# M03-L20: GatingNetwork for FuseMoE Expert Routing

```yaml
metadata:
  task_id: M03-L20
  version: 2.2.0
  status: completed
  layer: logic
  sequence: 20
  implements: PRD-FUSION (Laplace-smoothed gating)
  depends_on:
    - M03-F02  # dimensions module (TOTAL_CONCATENATED=8320, NUM_EXPERTS=8)
    - M03-F14  # FusionConfig struct (temperature, laplace_alpha, num_experts, top_k)
  blocks:
    - M03-L21  # Expert Networks (needs routing weights)
    - M03-L22  # FuseMoE Router (uses this gating network)
  estimated_hours: 3
  actual_hours: 4
  completed: 2026-01-01
  updated: 2026-01-01
  verified_by: sherlock-holmes
```

---

## CRITICAL CONTEXT FOR IMPLEMENTING AGENT

**You are implementing the neural network gating layer that routes 8320D concatenated embeddings to 8 expert networks.** The gating network produces probability weights over experts using temperature-scaled softmax with Laplace smoothing.

### Current Codebase State (Audited 2026-01-01)

**Existing Files You MUST Use:**
- `crates/context-graph-embeddings/src/types/dimensions.rs` - Contains `TOTAL_CONCATENATED=8320`, `NUM_EXPERTS=8`, `FUSED_OUTPUT=1536`
- `crates/context-graph-embeddings/src/config.rs` - Contains `FusionConfig` with `temperature`, `laplace_alpha`, `num_experts`, `top_k`
- `crates/context-graph-embeddings/src/error.rs` - Contains `EmbeddingError` enum with `ConfigError`, `InferenceError`
- `crates/context-graph-embeddings/src/lib.rs` - Module exports, includes `pub use config::FusionConfig`

**Files You MUST Create:**
- `crates/context-graph-embeddings/src/fusion/mod.rs` - Module declaration
- `crates/context-graph-embeddings/src/fusion/gating.rs` - GatingNetwork implementation

**NO fusion/ directory exists yet.** You must create it.

**IMPORTANT: The original task specified `candle` tensor operations. This is INCORRECT.**
- The `candle` feature exists in Cargo.toml but is NOT enabled and NOT a dependency
- You MUST implement using pure Rust `Vec<f32>` operations
- NO external ML framework dependencies

### Constitution.yaml Requirements (Source of Truth)

```yaml
embeddings:
  fusion:
    fuse_moe:
      top_k: 4           # NOT 2! constitution.yaml line 322
      laplace_alpha: 0.01

neuromod:
  Serotonin:
    param: fuse_moe.top_k
    range: "[2,8]"       # top_k can be modulated between 2-8
  Noradrenaline:
    param: attention.temp
    range: "[0.5,2]"     # temperature can be modulated
```

### Key Dimensions (from dimensions.rs - ALREADY EXISTS)

```rust
pub const TOTAL_CONCATENATED: usize = 8320;  // Input to gating network
pub const NUM_EXPERTS: usize = 8;            // Output dimension
pub const FUSED_OUTPUT: usize = 1536;        // Final embedding dimension
```

### FusionConfig Fields (from config.rs - ALREADY EXISTS)

```rust
pub struct FusionConfig {
    pub num_experts: usize,      // Default: 8
    pub top_k: usize,            // Default: 4 (NOT 2!)
    pub output_dim: usize,       // Default: 1536
    pub expert_hidden_dim: usize, // Default: 4096
    pub load_balance_coef: f32,  // Default: 0.01
    pub capacity_factor: f32,    // Default: 1.25
    pub temperature: f32,        // Default: 1.0
    pub noise_std: f32,          // Default: 0.0
    pub laplace_alpha: f32,      // Default: 0.01
}
```

---

## WHAT YOU ARE BUILDING

A `GatingNetwork` struct that:
1. Takes 8320D concatenated embedding input
2. Applies LayerNorm for input stability
3. Projects to 8 dimensions (one per expert)
4. Applies temperature-scaled softmax
5. Optionally injects noise during training
6. Applies Laplace smoothing to prevent zero probabilities

### Mathematical Operations

**LayerNorm:**
```
y = (x - mean(x)) / sqrt(var(x) + eps) * weight + bias
```

**Temperature-Scaled Softmax:**
```
p_i = exp(logit_i / temperature) / sum(exp(logit_j / temperature))
```

**Laplace Smoothing:**
```
p_smoothed = (p + alpha) / (1 + alpha * K)
where K = num_experts = 8
```

---

## FILE TO CREATE: `crates/context-graph-embeddings/src/fusion/mod.rs`

```rust
//! FuseMoE fusion layer components.
//!
//! This module implements the Mixture-of-Experts fusion for combining
//! 12 model embeddings into a unified 1536D representation.

pub mod gating;

pub use gating::{GatingNetwork, LayerNorm, Linear};
```

---

## FILE TO CREATE: `crates/context-graph-embeddings/src/fusion/gating.rs`

### Required Struct Signatures

```rust
use crate::config::FusionConfig;
use crate::error::{EmbeddingError, EmbeddingResult};
use crate::types::dimensions::{TOTAL_CONCATENATED, NUM_EXPERTS};

/// Layer normalization for input stability.
/// Normalizes across the feature dimension with learnable affine parameters.
pub struct LayerNorm {
    /// Learnable scale parameter (initialized to 1.0)
    weight: Vec<f32>,
    /// Learnable shift parameter (initialized to 0.0)
    bias: Vec<f32>,
    /// Numerical stability constant
    eps: f32,
    /// Feature dimension
    dim: usize,
}

/// Linear projection layer (weight matrix + optional bias).
pub struct Linear {
    /// Weight matrix [output_dim, input_dim] stored row-major
    weight: Vec<f32>,
    /// Optional bias vector [output_dim]
    bias: Option<Vec<f32>>,
    /// Input dimension
    input_dim: usize,
    /// Output dimension
    output_dim: usize,
}

/// Gating network for FuseMoE expert routing.
///
/// Routes 8320D concatenated embeddings to 8 experts using
/// temperature-scaled softmax with Laplace smoothing.
pub struct GatingNetwork {
    layer_norm: LayerNorm,
    projection: Linear,
    num_experts: usize,
    temperature: f32,
    laplace_alpha: f32,
}

impl GatingNetwork {
    /// Create new gating network from config.
    ///
    /// # Arguments
    /// * `config` - FusionConfig containing temperature, laplace_alpha, num_experts
    ///
    /// # Errors
    /// * `EmbeddingError::ConfigError` if num_experts is 0
    /// * `EmbeddingError::ConfigError` if temperature <= 0
    pub fn new(config: &FusionConfig) -> EmbeddingResult<Self>;

    /// Create with explicit parameters (for testing).
    ///
    /// # Arguments
    /// * `input_dim` - Input dimension (must be TOTAL_CONCATENATED=8320)
    /// * `num_experts` - Number of experts (must be NUM_EXPERTS=8)
    /// * `temperature` - Softmax temperature (must be > 0)
    /// * `laplace_alpha` - Laplace smoothing alpha (must be >= 0)
    pub fn with_params(
        input_dim: usize,
        num_experts: usize,
        temperature: f32,
        laplace_alpha: f32,
    ) -> EmbeddingResult<Self>;

    /// Forward pass producing expert probabilities.
    ///
    /// Input: [batch_size, 8320]
    /// Output: [batch_size, 8] with softmax probabilities summing to 1.0
    ///
    /// # Errors
    /// * `EmbeddingError::DimensionMismatch` if input dimension != 8320
    pub fn forward(&self, input: &[f32], batch_size: usize) -> EmbeddingResult<Vec<f32>>;

    /// Forward with noise injection for training load balancing.
    ///
    /// Adds Gaussian noise to logits before softmax for exploration.
    /// Set noise_std=0.0 for deterministic inference.
    ///
    /// # Arguments
    /// * `input` - Flattened [batch_size * 8320] input
    /// * `batch_size` - Number of samples in batch
    /// * `noise_std` - Standard deviation of Gaussian noise
    pub fn forward_with_noise(
        &self,
        input: &[f32],
        batch_size: usize,
        noise_std: f32,
    ) -> EmbeddingResult<Vec<f32>>;

    /// Apply Laplace smoothing to probabilities.
    ///
    /// Formula: (p + alpha) / (1 + alpha * K)
    /// where K = num_experts
    ///
    /// Prevents zero probabilities for numerical stability.
    pub fn apply_laplace_smoothing(&self, probs: &mut [f32]);

    /// Get number of experts.
    #[inline]
    pub fn num_experts(&self) -> usize;

    /// Get temperature parameter.
    #[inline]
    pub fn temperature(&self) -> f32;
}
```

---

## IMPLEMENTATION CONSTRAINTS

1. **Input dimension: 8320** (use `TOTAL_CONCATENATED` constant, NOT hardcoded)
2. **Output dimension: 8** (use `NUM_EXPERTS` constant, NOT hardcoded)
3. **Softmax probabilities MUST sum to 1.0** (within f32 epsilon tolerance)
4. **Laplace smoothing formula: (p + alpha) / (1 + alpha * K)** where K=8
5. **Temperature scaling: softmax(x / temperature)**
6. **LayerNorm with eps=1e-5** for numerical stability
7. **NO candle/ONNX dependencies** - use pure Rust Vec<f32> operations
8. **NO mock data in tests** - test with real computations
9. **NO backwards compatibility** - fail fast with clear errors

---

## VALIDATION CRITERIA (ALL MUST PASS)

| Criterion | How to Verify |
|-----------|---------------|
| `cargo check` passes | Run `cargo check -p context-graph-embeddings` |
| `cargo clippy` passes | Run `cargo clippy -p context-graph-embeddings -- -D warnings` |
| Output shape [batch, 8] | Unit test: `assert_eq!(output.len(), batch_size * 8)` |
| Softmax sums to 1.0 | Unit test: `assert!((sum - 1.0).abs() < 1e-5)` for each batch |
| Laplace smoothing valid | Unit test: verify no probabilities are 0.0 after smoothing |
| temperature=1.0 equals standard softmax | Unit test: compare with manual softmax calculation |
| Higher temperature = more uniform | Unit test: entropy increases with temperature |
| Noise injection varies output | Unit test: multiple calls with noise_std>0 produce different outputs |
| Zero noise = deterministic | Unit test: multiple calls with noise_std=0 produce identical outputs |

---

## FULL STATE VERIFICATION (MANDATORY)

After completing the logic, you MUST perform these verification steps:

### 1. Define Source of Truth
The source of truth is the **output probability vector** from `GatingNetwork::forward()`.
- Location: Return value of `forward()` method
- Expected properties:
  - Length = `batch_size * NUM_EXPERTS`
  - All values in [0, 1]
  - Each batch's 8 values sum to 1.0

### 2. Execute & Inspect
Run the logic, then immediately inspect:

```rust
#[test]
fn verify_forward_output_correctness() {
    let config = FusionConfig::default();
    let gating = GatingNetwork::new(&config).expect("Failed to create gating network");

    // Create deterministic test input
    let batch_size = 2;
    let input: Vec<f32> = (0..batch_size * TOTAL_CONCATENATED)
        .map(|i| (i as f32 * 0.001).sin())
        .collect();

    // Execute
    let output = gating.forward(&input, batch_size)
        .expect("Forward pass failed");

    // INSPECT: Print raw output
    println!("=== SOURCE OF TRUTH INSPECTION ===");
    println!("Output length: {} (expected: {})", output.len(), batch_size * NUM_EXPERTS);

    for batch in 0..batch_size {
        let start = batch * NUM_EXPERTS;
        let end = start + NUM_EXPERTS;
        let batch_probs = &output[start..end];
        let sum: f32 = batch_probs.iter().sum();
        println!("Batch {}: {:?}", batch, batch_probs);
        println!("Batch {} sum: {} (expected: 1.0)", batch, sum);

        // VERIFY
        assert!((sum - 1.0).abs() < 1e-5, "Probabilities must sum to 1.0");
        for p in batch_probs {
            assert!(*p >= 0.0 && *p <= 1.0, "Probability out of range: {}", p);
        }
    }
    println!("=== VERIFICATION PASSED ===");
}
```

### 3. Edge Case Audit (3 Required)

You MUST manually simulate and print state for these 3 edge cases:

#### Edge Case 1: Empty Input (Expect Error)
```rust
#[test]
fn edge_case_empty_input() {
    println!("=== EDGE CASE 1: Empty Input ===");
    let gating = GatingNetwork::with_params(TOTAL_CONCATENATED, NUM_EXPERTS, 1.0, 0.01).unwrap();

    println!("BEFORE: input.len() = 0, batch_size = 0");
    let result = gating.forward(&[], 0);
    println!("AFTER: result = {:?}", result);

    // Should error or return empty, not panic
    assert!(result.is_err() || result.unwrap().is_empty());
    println!("Edge Case 1 PASSED: Empty input handled correctly");
}
```

#### Edge Case 2: Maximum Temperature (Uniform Distribution)
```rust
#[test]
fn edge_case_max_temperature() {
    println!("=== EDGE CASE 2: Maximum Temperature ===");
    let gating = GatingNetwork::with_params(TOTAL_CONCATENATED, NUM_EXPERTS, 100.0, 0.01).unwrap();

    let input: Vec<f32> = vec![1.0; TOTAL_CONCATENATED];

    println!("BEFORE: temperature = 100.0, input = uniform 1.0");
    let output = gating.forward(&input, 1).unwrap();
    println!("AFTER: output = {:?}", output);

    // High temperature should produce near-uniform distribution
    let expected_uniform = 1.0 / NUM_EXPERTS as f32;
    for p in &output {
        let diff = (*p - expected_uniform).abs();
        println!("  prob={:.6}, diff from uniform={:.6}", p, diff);
        assert!(diff < 0.1, "High temp should be near-uniform");
    }
    println!("Edge Case 2 PASSED: High temperature produces uniform distribution");
}
```

#### Edge Case 3: Zero Laplace Alpha (No Smoothing)
```rust
#[test]
fn edge_case_zero_laplace() {
    println!("=== EDGE CASE 3: Zero Laplace Alpha ===");
    let gating = GatingNetwork::with_params(TOTAL_CONCATENATED, NUM_EXPERTS, 0.1, 0.0).unwrap();

    // Create input that would produce extreme logits
    let mut input: Vec<f32> = vec![0.0; TOTAL_CONCATENATED];
    input[0] = 1000.0; // Extreme value

    println!("BEFORE: laplace_alpha = 0.0, extreme input at index 0");
    let output = gating.forward(&input, 1).unwrap();
    println!("AFTER: output = {:?}", output);

    // Without smoothing, one expert may dominate completely
    let max_prob = output.iter().cloned().fold(f32::NEG_INFINITY, f32::max);
    println!("Max probability: {}", max_prob);
    assert!(max_prob > 0.5, "Low temp + no smoothing should concentrate probability");
    println!("Edge Case 3 PASSED: Zero laplace allows probability concentration");
}
```

### 4. Evidence of Success

After all tests pass, run and capture this log:

```bash
cargo test -p context-graph-embeddings gating -- --nocapture 2>&1 | tee /tmp/gating_test_output.log
```

The log MUST show:
- All "=== SOURCE OF TRUTH INSPECTION ===" sections
- All "Edge Case N PASSED" messages
- "test result: ok" at the end

---

## UPDATE lib.rs TO EXPORT FUSION MODULE

Add to `crates/context-graph-embeddings/src/lib.rs`:

```rust
pub mod fusion;

pub use fusion::{GatingNetwork, LayerNorm, Linear};
```

---

## FINAL VERIFICATION WITH SHERLOCK-HOLMES AGENT

After completing implementation, you MUST spawn a `sherlock-holmes` subagent with this prompt:

```
Investigate the M03-L20 GatingNetwork implementation:

1. VERIFY all files exist:
   - crates/context-graph-embeddings/src/fusion/mod.rs
   - crates/context-graph-embeddings/src/fusion/gating.rs

2. VERIFY cargo check passes:
   - Run: cargo check -p context-graph-embeddings

3. VERIFY cargo test passes:
   - Run: cargo test -p context-graph-embeddings gating -- --nocapture

4. VERIFY constraints are met:
   - Input dimension uses TOTAL_CONCATENATED constant (not hardcoded 8320)
   - Output dimension uses NUM_EXPERTS constant (not hardcoded 8)
   - Softmax probabilities sum to 1.0
   - Laplace smoothing formula is (p + alpha) / (1 + alpha * K)
   - Temperature scaling is softmax(x / temperature)
   - LayerNorm uses eps=1e-5

5. REPORT any issues found and their exact locations.
```

Fix ANY issues identified by the sherlock-holmes agent before marking this task complete.

---

## ANTI-PATTERNS TO AVOID

| Anti-Pattern | Correct Approach |
|--------------|------------------|
| Hardcoded 8320 | Use `TOTAL_CONCATENATED` constant |
| Hardcoded 8 experts | Use `NUM_EXPERTS` constant |
| Mock data in tests | Use real computed values |
| Fallback on error | Return `EmbeddingError`, fail fast |
| unwrap() in implementation | Use `expect()` or `?` operator |
| Skip edge case tests | All 3 edge cases MUST be tested |
| Use candle/Tensor types | Use pure Rust Vec<f32> |

---

## DEPENDENCIES

### Cargo.toml Changes (NONE REQUIRED)
The current dependencies are sufficient:
- `rand = "0.8"` - Already present for noise injection
- No candle/ONNX dependencies needed

### Import Requirements
```rust
use rand::Rng;
use crate::config::FusionConfig;
use crate::error::{EmbeddingError, EmbeddingResult};
use crate::types::dimensions::{TOTAL_CONCATENATED, NUM_EXPERTS};
```

---

## DISCREPANCIES FROM ORIGINAL TASK (CORRECTED)

| Original | Corrected | Reason |
|----------|-----------|--------|
| Use candle Tensor | Use Vec<f32> | candle is not a dependency |
| PRD-EMB-004 reference | PRD-FUSION reference | Better matches traceability matrix |
| No laplace_alpha in struct | Added laplace_alpha field | Required for Laplace smoothing |
| constructor(input_dim, num_experts, temp) | constructor(config: &FusionConfig) | Use existing FusionConfig |

---

## IMPLEMENTATION SUMMARY (COMPLETED)

### Files Created
| File | Lines | Description |
|------|-------|-------------|
| `crates/context-graph-embeddings/src/fusion/mod.rs` | 39 | Module exports |
| `crates/context-graph-embeddings/src/fusion/gating.rs` | 1661 | GatingNetwork, LayerNorm, Linear implementations |
| `crates/context-graph-embeddings/src/fusion/experts.rs` | 1208 | Expert, ExpertPool, Activation implementations |

### Implementation Evidence
- **GatingNetwork**: LayerNorm(8320) → Linear(8320→8) → Softmax(T-scaled) → Laplace(α=0.01) → TopK(4)
- **ExpertPool**: 8 experts, each FFN: Linear(8320→4096) → GELU → Linear(4096→1536)
- **Total Parameters**: ~323M (40.4M per expert × 8)

### Constitution Compliance (Verified)
| Parameter | Constitution Value | Implementation |
|-----------|-------------------|----------------|
| fuse_moe.num_experts | 8 | NUM_EXPERTS=8 ✓ |
| fuse_moe.top_k | 4 | TOP_K_EXPERTS=4 ✓ |
| fuse_moe.laplace_alpha | 0.01 | default_laplace_alpha()=0.01 ✓ |
| dimensions.total_concatenated | 8320 | TOTAL_CONCATENATED=8320 ✓ |
| dimensions.fused_output | 1536 | FUSED_OUTPUT=1536 ✓ |

### Test Coverage
| Category | Test Count | Location |
|----------|------------|----------|
| LayerNorm | 14 | gating.rs:1005-1143 |
| Linear | 11 | gating.rs:1149-1244 |
| GatingNetwork | 25+ | gating.rs:1250-1509 |
| Edge Cases | 8 | gating.rs:1514-1578 |
| Integration | 5 | gating.rs:1584-1660 |
| Activation | 5 | experts.rs:653-706 |
| Expert | 9 | experts.rs:712-803 |
| ExpertPool | 17 | experts.rs:808-1018 |
| **Total** | **100+** | Real tests, no mocks |

### Verification Command
```bash
cargo test -p context-graph-embeddings gating -- --nocapture
cargo test -p context-graph-embeddings expert -- --nocapture
```

---

*Task Completed: 2026-01-01*
*Version: 2.2.0*
*Verified By: sherlock-holmes forensic investigation*
*Implementation Status: COMPLETE - All 9 requirements verified NOT GUILTY*
