# TASK-M02-023: Implement Secondary Index Operations

```xml
<task_spec id="TASK-M02-023" version="2.1">
<metadata>
  <title>Implement Secondary Index Operations</title>
  <status>verified</status>
  <layer>surface</layer>
  <module>module-02</module>
  <sequence>23</sequence>
  <priority>high</priority>
  <estimated_hours>3</estimated_hours>
  <actual_hours>2.5</actual_hours>
  <implements>
    <item>TECH-CORE-002 Section 3: Storage specification</item>
    <item>REQ-CORE-005: Storage requirements</item>
  </implements>
  <depends_on>
    <task_ref status="complete">TASK-M02-017</task_ref>
  </depends_on>
  <estimated_complexity>medium</estimated_complexity>
  <verification>
    <date>2025-12-31</date>
    <agent>sherlock-holmes</agent>
    <verdict>INNOCENT</verdict>
    <tests_passed>29/29</tests_passed>
    <clippy_warnings>0</clippy_warnings>
  </verification>
</metadata>
```

## ✅ TASK COMPLETE - Sherlock Holmes Verified

**Verification Date:** 2025-12-31
**Verdict:** INNOCENT (All requirements met)

### Verification Summary

| Check | Result |
|-------|--------|
| Build | PASS (0 errors) |
| Tests | 29/29 PASS |
| Clippy | 0 warnings |
| Methods | All 4 present |
| Edge Cases | All covered |
| Integration | Verified |

### Implementation Files

| File | Purpose | Status |
|------|---------|--------|
| `rocksdb_backend/index_ops.rs` | 4 query methods (366 lines) | ✅ Complete |
| `rocksdb_backend/tests_index.rs` | 29 tests (697 lines) | ✅ Complete |
| `rocksdb_backend/mod.rs` | Module exports | ✅ Updated |

### Methods Implemented

1. `get_nodes_by_quadrant(quadrant, limit, offset)` - Johari quadrant queries
2. `get_nodes_by_tag(tag, limit, offset)` - Tag-based queries
3. `get_nodes_by_source(source, limit, offset)` - Source-based queries
4. `get_nodes_in_time_range(start, end, limit, offset)` - Temporal queries

---

## Historical Context (For Reference)

### Codebase State (as of 2025-12-31)

The storage module uses a modular directory structure:

| Original (WRONG) | Actual (CORRECT) |
|------------------|------------------|
| `crates/context-graph-storage/src/rocksdb_backend.rs` | **Does not exist** - replaced by module directory |
| N/A | `crates/context-graph-storage/src/rocksdb_backend/` (directory with submodules) |
| `crates/context-graph-core/src/johari.rs` | `crates/context-graph-core/src/types/johari/quadrant.rs` |

### Actual Module Structure

```
crates/context-graph-storage/src/
├── lib.rs                    # Main exports
├── column_families.rs        # CF definitions with cf_names::* constants
├── serialization.rs          # serialize_*/deserialize_* functions
├── indexes.rs                # STUB FILE - needs implementation
├── memex.rs                  # STUB - future Memex trait
└── rocksdb_backend/
    ├── mod.rs                # Re-exports: RocksDbMemex, StorageError, RocksDbConfig
    ├── config.rs             # RocksDbConfig struct
    ├── error.rs              # StorageError enum
    ├── core.rs               # RocksDbMemex struct: open(), get_cf(), health_check(), etc.
    ├── helpers.rs            # format_temporal_key(), format_tag_key(), format_source_key()
    ├── node_ops.rs           # store_node(), get_node(), update_node(), delete_node()
    ├── edge_ops.rs           # store_edge(), get_edge(), update_edge(), delete_edge(), etc.
    └── tests_*.rs            # Test files (tests_node.rs, tests_edge.rs, etc.)
```

## What You Must Build

Add secondary index query methods to `RocksDbMemex`. These methods query the indexes that are **already populated** by `store_node()` in `node_ops.rs`.

### Index Key Formats (Already Implemented in helpers.rs)

| Column Family | Key Format | Helper Function |
|---------------|------------|-----------------|
| `johari_{quadrant}` | `16-byte NodeId` | `serialize_uuid()` |
| `tags` | `{tag_string}:{16-byte NodeId}` | `format_tag_key()` |
| `sources` | `{source_string}:{16-byte NodeId}` | `format_source_key()` |
| `temporal` | `{8-byte timestamp BE}:{16-byte NodeId}` | `format_temporal_key()` |

## Implementation Requirements

### File to Create: `crates/context-graph-storage/src/rocksdb_backend/index_ops.rs`

```rust
//! Secondary index query operations for RocksDB backend.
//!
//! Provides query methods that leverage indexes created during node storage.
//! All operations use RocksDB iterators for efficient scanning.

use chrono::{DateTime, Utc};
use rocksdb::IteratorMode;

use crate::column_families::cf_names;
use crate::serialization::deserialize_uuid;
use context_graph_core::types::{JohariQuadrant, NodeId};

use super::core::RocksDbMemex;
use super::error::StorageError;
use super::helpers::{format_source_key, format_tag_key};

impl RocksDbMemex {
    /// Gets all node IDs in a specific Johari quadrant.
    ///
    /// Uses the johari_{quadrant} column family full scan.
    /// Keys in Johari CFs are raw 16-byte NodeId.
    ///
    /// # Arguments
    /// * `quadrant` - The Johari quadrant to query
    /// * `limit` - Maximum number of results (None = unlimited)
    /// * `offset` - Number of results to skip
    ///
    /// # Returns
    /// * `Ok(Vec<NodeId>)` - List of node IDs in the quadrant
    /// * Empty Vec if no nodes in quadrant (NOT an error)
    ///
    /// # Performance
    /// Target: O(n) where n = nodes in quadrant. Uses iterator, not full load.
    pub fn get_nodes_by_quadrant(
        &self,
        quadrant: JohariQuadrant,
        limit: Option<usize>,
        offset: usize,
    ) -> Result<Vec<NodeId>, StorageError> {
        let cf = self.get_cf(quadrant.column_family())?;
        let iter = self.db.iterator_cf(cf, IteratorMode::Start);

        let mut results = Vec::new();
        let mut skipped = 0;

        for item in iter {
            let (key, _) = item.map_err(|e| StorageError::ReadFailed(e.to_string()))?;

            // Handle offset
            if skipped < offset {
                skipped += 1;
                continue;
            }

            // Parse 16-byte key as UUID
            if key.len() == 16 {
                let node_id = deserialize_uuid(&key)?;
                results.push(node_id);
            }

            // Handle limit
            if let Some(max) = limit {
                if results.len() >= max {
                    break;
                }
            }
        }

        Ok(results)
    }

    /// Gets all node IDs with a specific tag.
    ///
    /// Uses the tags column family with prefix scan.
    /// Key format: `{tag_string}:{16-byte NodeId}`
    ///
    /// # Arguments
    /// * `tag` - Tag to search for (exact match)
    /// * `limit` - Maximum results (None = unlimited)
    /// * `offset` - Results to skip
    ///
    /// # Returns
    /// Empty Vec if no nodes have this tag (NOT an error)
    pub fn get_nodes_by_tag(
        &self,
        tag: &str,
        limit: Option<usize>,
        offset: usize,
    ) -> Result<Vec<NodeId>, StorageError> {
        let cf = self.get_cf(cf_names::TAGS)?;

        // Create prefix: tag_bytes + ':'
        let mut prefix = Vec::with_capacity(tag.len() + 1);
        prefix.extend_from_slice(tag.as_bytes());
        prefix.push(b':');

        let iter = self.db.prefix_iterator_cf(cf, &prefix);

        let mut results = Vec::new();
        let mut skipped = 0;

        for item in iter {
            let (key, _) = item.map_err(|e| StorageError::ReadFailed(e.to_string()))?;

            // Stop if prefix no longer matches
            if !key.starts_with(&prefix) {
                break;
            }

            // Handle offset
            if skipped < offset {
                skipped += 1;
                continue;
            }

            // Extract NodeId from key (16 bytes after prefix)
            let uuid_start = prefix.len();
            if key.len() >= uuid_start + 16 {
                let node_id = deserialize_uuid(&key[uuid_start..])?;
                results.push(node_id);
            }

            // Handle limit
            if let Some(max) = limit {
                if results.len() >= max {
                    break;
                }
            }
        }

        Ok(results)
    }

    /// Gets all node IDs from a specific source.
    ///
    /// Uses the sources column family with prefix scan.
    /// Key format: `{source_string}:{16-byte NodeId}`
    pub fn get_nodes_by_source(
        &self,
        source: &str,
        limit: Option<usize>,
        offset: usize,
    ) -> Result<Vec<NodeId>, StorageError> {
        let cf = self.get_cf(cf_names::SOURCES)?;

        // Create prefix: source_bytes + ':'
        let mut prefix = Vec::with_capacity(source.len() + 1);
        prefix.extend_from_slice(source.as_bytes());
        prefix.push(b':');

        let iter = self.db.prefix_iterator_cf(cf, &prefix);

        let mut results = Vec::new();
        let mut skipped = 0;

        for item in iter {
            let (key, _) = item.map_err(|e| StorageError::ReadFailed(e.to_string()))?;

            if !key.starts_with(&prefix) {
                break;
            }

            if skipped < offset {
                skipped += 1;
                continue;
            }

            let uuid_start = prefix.len();
            if key.len() >= uuid_start + 16 {
                let node_id = deserialize_uuid(&key[uuid_start..])?;
                results.push(node_id);
            }

            if let Some(max) = limit {
                if results.len() >= max {
                    break;
                }
            }
        }

        Ok(results)
    }

    /// Gets all node IDs created within a time range.
    ///
    /// Uses the temporal column family with range scan.
    /// Key format: `{8-byte timestamp millis BE}:{16-byte NodeId}`
    ///
    /// # Arguments
    /// * `start` - Start of time range (inclusive)
    /// * `end` - End of time range (exclusive)
    /// * `limit` - Maximum results
    /// * `offset` - Results to skip
    ///
    /// # Returns
    /// Nodes ordered by creation time (oldest first due to BE ordering)
    pub fn get_nodes_in_time_range(
        &self,
        start: DateTime<Utc>,
        end: DateTime<Utc>,
        limit: Option<usize>,
        offset: usize,
    ) -> Result<Vec<NodeId>, StorageError> {
        let cf = self.get_cf(cf_names::TEMPORAL)?;

        // Create start key (timestamp only, 8 bytes)
        let start_millis = start.timestamp_millis() as u64;
        let end_millis = end.timestamp_millis() as u64;
        let start_key = start_millis.to_be_bytes();

        let iter = self.db.iterator_cf(
            cf,
            IteratorMode::From(&start_key, rocksdb::Direction::Forward),
        );

        let mut results = Vec::new();
        let mut skipped = 0;

        for item in iter {
            let (key, _) = item.map_err(|e| StorageError::ReadFailed(e.to_string()))?;

            // Key must be 24 bytes: 8 timestamp + 16 UUID
            if key.len() != 24 {
                continue;
            }

            // Extract timestamp from first 8 bytes
            let ts_bytes: [u8; 8] = key[0..8].try_into().unwrap();
            let key_millis = u64::from_be_bytes(ts_bytes);

            // Stop if past end time
            if key_millis >= end_millis {
                break;
            }

            // Handle offset
            if skipped < offset {
                skipped += 1;
                continue;
            }

            // Extract NodeId from bytes 8-24
            let node_id = deserialize_uuid(&key[8..24])?;
            results.push(node_id);

            // Handle limit
            if let Some(max) = limit {
                if results.len() >= max {
                    break;
                }
            }
        }

        Ok(results)
    }
}
```

### File to Create: `crates/context-graph-storage/src/rocksdb_backend/tests_index.rs`

```rust
//! Secondary index operation tests.
//!
//! Tests use REAL data stored in RocksDB - NO mocks per constitution.yaml.
//! Each test stores nodes via store_node() then queries via index methods.

use chrono::{Duration, Utc};
use tempfile::TempDir;

use super::core::RocksDbMemex;
use super::tests_node::{create_temp_db, create_valid_test_node, create_node_with_tags};
use context_graph_core::types::JohariQuadrant;

// =========================================================================
// get_nodes_by_quadrant Tests
// =========================================================================

#[test]
fn test_get_nodes_by_quadrant_empty() {
    println!("=== TEST: get_nodes_by_quadrant on empty quadrant ===");
    let (_tmp, db) = create_temp_db();

    let result = db.get_nodes_by_quadrant(JohariQuadrant::Blind, None, 0);

    assert!(result.is_ok());
    assert!(result.unwrap().is_empty(), "Empty quadrant should return empty Vec, NOT error");
}

#[test]
fn test_get_nodes_by_quadrant_finds_nodes() {
    println!("=== TEST: get_nodes_by_quadrant finds stored nodes ===");
    let (_tmp, db) = create_temp_db();

    // Store 3 nodes in Open quadrant
    let mut node1 = create_valid_test_node();
    node1.quadrant = JohariQuadrant::Open;
    db.store_node(&node1).expect("store 1");

    let mut node2 = create_valid_test_node();
    node2.quadrant = JohariQuadrant::Open;
    db.store_node(&node2).expect("store 2");

    let mut node3 = create_valid_test_node();
    node3.quadrant = JohariQuadrant::Hidden;
    db.store_node(&node3).expect("store 3");

    let open_nodes = db.get_nodes_by_quadrant(JohariQuadrant::Open, None, 0).unwrap();
    let hidden_nodes = db.get_nodes_by_quadrant(JohariQuadrant::Hidden, None, 0).unwrap();

    println!("RESULT: Open={}, Hidden={}", open_nodes.len(), hidden_nodes.len());
    assert_eq!(open_nodes.len(), 2);
    assert_eq!(hidden_nodes.len(), 1);
    assert!(open_nodes.contains(&node1.id));
    assert!(open_nodes.contains(&node2.id));
    assert!(hidden_nodes.contains(&node3.id));
}

#[test]
fn test_get_nodes_by_quadrant_with_limit() {
    let (_tmp, db) = create_temp_db();

    for _ in 0..5 {
        let mut node = create_valid_test_node();
        node.quadrant = JohariQuadrant::Open;
        db.store_node(&node).unwrap();
    }

    let result = db.get_nodes_by_quadrant(JohariQuadrant::Open, Some(3), 0).unwrap();
    assert_eq!(result.len(), 3, "Limit should cap results at 3");
}

#[test]
fn test_get_nodes_by_quadrant_with_offset() {
    let (_tmp, db) = create_temp_db();

    for _ in 0..5 {
        let mut node = create_valid_test_node();
        node.quadrant = JohariQuadrant::Open;
        db.store_node(&node).unwrap();
    }

    let result = db.get_nodes_by_quadrant(JohariQuadrant::Open, None, 2).unwrap();
    assert_eq!(result.len(), 3, "Offset 2 from 5 nodes = 3 results");
}

// =========================================================================
// get_nodes_by_tag Tests
// =========================================================================

#[test]
fn test_get_nodes_by_tag_empty() {
    let (_tmp, db) = create_temp_db();

    let result = db.get_nodes_by_tag("nonexistent", None, 0);
    assert!(result.is_ok());
    assert!(result.unwrap().is_empty());
}

#[test]
fn test_get_nodes_by_tag_finds_nodes() {
    println!("=== TEST: get_nodes_by_tag finds tagged nodes ===");
    let (_tmp, db) = create_temp_db();

    let node1 = create_node_with_tags(vec!["important", "work"]);
    db.store_node(&node1).unwrap();

    let node2 = create_node_with_tags(vec!["important", "personal"]);
    db.store_node(&node2).unwrap();

    let node3 = create_node_with_tags(vec!["personal"]);
    db.store_node(&node3).unwrap();

    let important = db.get_nodes_by_tag("important", None, 0).unwrap();
    let personal = db.get_nodes_by_tag("personal", None, 0).unwrap();
    let work = db.get_nodes_by_tag("work", None, 0).unwrap();

    println!("RESULT: important={}, personal={}, work={}",
        important.len(), personal.len(), work.len());
    assert_eq!(important.len(), 2);
    assert_eq!(personal.len(), 2);
    assert_eq!(work.len(), 1);
}

#[test]
fn test_get_nodes_by_tag_with_pagination() {
    let (_tmp, db) = create_temp_db();

    for _ in 0..10 {
        let node = create_node_with_tags(vec!["paginated"]);
        db.store_node(&node).unwrap();
    }

    // First page
    let page1 = db.get_nodes_by_tag("paginated", Some(3), 0).unwrap();
    assert_eq!(page1.len(), 3);

    // Second page
    let page2 = db.get_nodes_by_tag("paginated", Some(3), 3).unwrap();
    assert_eq!(page2.len(), 3);

    // No overlap
    for id in &page1 {
        assert!(!page2.contains(id), "Pages should not overlap");
    }
}

// =========================================================================
// get_nodes_by_source Tests
// =========================================================================

#[test]
fn test_get_nodes_by_source_empty() {
    let (_tmp, db) = create_temp_db();

    let result = db.get_nodes_by_source("unknown-source", None, 0);
    assert!(result.is_ok());
    assert!(result.unwrap().is_empty());
}

#[test]
fn test_get_nodes_by_source_finds_nodes() {
    println!("=== TEST: get_nodes_by_source finds nodes ===");
    let (_tmp, db) = create_temp_db();

    let mut node1 = create_valid_test_node();
    node1.metadata.source = Some("api-gateway".to_string());
    db.store_node(&node1).unwrap();

    let mut node2 = create_valid_test_node();
    node2.metadata.source = Some("api-gateway".to_string());
    db.store_node(&node2).unwrap();

    let mut node3 = create_valid_test_node();
    node3.metadata.source = Some("web-scraper".to_string());
    db.store_node(&node3).unwrap();

    let api_nodes = db.get_nodes_by_source("api-gateway", None, 0).unwrap();
    let web_nodes = db.get_nodes_by_source("web-scraper", None, 0).unwrap();

    println!("RESULT: api-gateway={}, web-scraper={}", api_nodes.len(), web_nodes.len());
    assert_eq!(api_nodes.len(), 2);
    assert_eq!(web_nodes.len(), 1);
}

// =========================================================================
// get_nodes_in_time_range Tests
// =========================================================================

#[test]
fn test_get_nodes_in_time_range_empty() {
    let (_tmp, db) = create_temp_db();

    let start = Utc::now() - Duration::hours(1);
    let end = Utc::now();

    let result = db.get_nodes_in_time_range(start, end, None, 0);
    assert!(result.is_ok());
    assert!(result.unwrap().is_empty());
}

#[test]
fn test_get_nodes_in_time_range_finds_nodes() {
    println!("=== TEST: get_nodes_in_time_range finds nodes ===");
    let (_tmp, db) = create_temp_db();

    // Store nodes (all created "now")
    let node1 = create_valid_test_node();
    let created_at = node1.created_at;
    db.store_node(&node1).unwrap();

    let node2 = create_valid_test_node();
    db.store_node(&node2).unwrap();

    // Query with range that includes all nodes
    let start = created_at - Duration::seconds(1);
    let end = Utc::now() + Duration::seconds(1);

    let result = db.get_nodes_in_time_range(start, end, None, 0).unwrap();

    println!("RESULT: Found {} nodes in time range", result.len());
    assert_eq!(result.len(), 2);
    assert!(result.contains(&node1.id));
    assert!(result.contains(&node2.id));
}

#[test]
fn test_get_nodes_in_time_range_respects_boundaries() {
    println!("=== TEST: Time range respects start/end boundaries ===");
    let (_tmp, db) = create_temp_db();

    let node = create_valid_test_node();
    let node_time = node.created_at;
    db.store_node(&node).unwrap();

    // Range BEFORE node
    let before = db.get_nodes_in_time_range(
        node_time - Duration::hours(2),
        node_time - Duration::hours(1),
        None, 0
    ).unwrap();
    assert!(before.is_empty(), "Range before node should be empty");

    // Range AFTER node
    let after = db.get_nodes_in_time_range(
        node_time + Duration::hours(1),
        node_time + Duration::hours(2),
        None, 0
    ).unwrap();
    assert!(after.is_empty(), "Range after node should be empty");

    // Range INCLUDING node
    let including = db.get_nodes_in_time_range(
        node_time - Duration::seconds(1),
        node_time + Duration::seconds(1),
        None, 0
    ).unwrap();
    assert_eq!(including.len(), 1, "Range including node should find it");
}

// =========================================================================
// Pagination Tests (Cross-Method)
// =========================================================================

#[test]
fn test_pagination_correctness() {
    println!("=== TEST: Pagination returns correct subsets ===");
    let (_tmp, db) = create_temp_db();

    // Store 10 nodes with same tag
    let mut all_ids = Vec::new();
    for _ in 0..10 {
        let node = create_node_with_tags(vec!["bulk"]);
        all_ids.push(node.id);
        db.store_node(&node).unwrap();
    }

    // Get all via no limit
    let all = db.get_nodes_by_tag("bulk", None, 0).unwrap();
    assert_eq!(all.len(), 10);

    // Get pages of 3
    let mut collected = Vec::new();
    for page in 0..4 {
        let offset = page * 3;
        let result = db.get_nodes_by_tag("bulk", Some(3), offset).unwrap();
        collected.extend(result);
    }

    // Should have collected all 10 (3+3+3+1)
    assert_eq!(collected.len(), 10);

    // All original IDs should be present
    for id in &all_ids {
        assert!(collected.contains(id), "All IDs should be collected via pagination");
    }
}

// =========================================================================
// Edge Case Tests
// =========================================================================

#[test]
fn edge_case_tag_with_special_chars() {
    println!("=== EDGE CASE: Tag with special characters ===");
    let (_tmp, db) = create_temp_db();

    // Test various special character tags
    for tag in &["tag:with:colons", "tag/with/slashes", "tag with spaces", "日本語"] {
        let node = create_node_with_tags(vec![tag]);
        db.store_node(&node).unwrap();

        let result = db.get_nodes_by_tag(tag, None, 0).unwrap();
        assert_eq!(result.len(), 1, "Should find node with tag '{}'", tag);
    }
}

#[test]
fn edge_case_offset_beyond_results() {
    let (_tmp, db) = create_temp_db();

    for _ in 0..3 {
        let mut node = create_valid_test_node();
        node.quadrant = JohariQuadrant::Open;
        db.store_node(&node).unwrap();
    }

    // Offset beyond total count
    let result = db.get_nodes_by_quadrant(JohariQuadrant::Open, None, 100).unwrap();
    assert!(result.is_empty(), "Offset beyond results should return empty Vec");
}

#[test]
fn edge_case_limit_zero() {
    let (_tmp, db) = create_temp_db();

    let node = create_valid_test_node();
    db.store_node(&node).unwrap();

    // Limit of 0 should return empty
    let result = db.get_nodes_by_quadrant(node.quadrant, Some(0), 0).unwrap();
    assert!(result.is_empty(), "Limit 0 should return empty Vec");
}
```

### Files to Modify

**1. `crates/context-graph-storage/src/rocksdb_backend/mod.rs`**

Add to the module declarations:
```rust
mod index_ops;
```

Add to the test modules:
```rust
#[cfg(test)]
mod tests_index;
```

**2. `crates/context-graph-storage/src/indexes.rs`**

Replace the stub with re-exports (or delete entirely and move the doc to mod.rs):
```rust
//! Secondary index operations.
//!
//! Index queries are implemented in `rocksdb_backend/index_ops.rs`
//! as methods on `RocksDbMemex`.
//!
//! Available queries:
//! - `get_nodes_by_quadrant(quadrant, limit, offset)`
//! - `get_nodes_by_tag(tag, limit, offset)`
//! - `get_nodes_by_source(source, limit, offset)`
//! - `get_nodes_in_time_range(start, end, limit, offset)`
```

## Verification Commands

```bash
# Build verification (MUST pass)
cargo build --package context-graph-storage

# Run index tests specifically
cargo test --package context-graph-storage index -- --nocapture

# Run all storage tests
cargo test --package context-graph-storage -- --nocapture

# Clippy (0 warnings required)
cargo clippy --package context-graph-storage -- -D warnings
```

## Full State Verification (REQUIRED)

After implementation, you MUST verify:

### 1. Source of Truth
The data is stored in RocksDB column families. After calling `store_node()`, entries exist in:
- `nodes` CF: Serialized MemoryNode
- `johari_{quadrant}` CF: 16-byte NodeId key
- `tags` CF: `{tag}:{NodeId}` key
- `sources` CF: `{source}:{NodeId}` key
- `temporal` CF: `{timestamp}:{NodeId}` key

### 2. Execute & Inspect
Run each test and verify actual RocksDB contents:
```rust
// After store_node(&node):
let cf_johari = db.get_cf(node.quadrant.column_family())?;
let exists = db.db().get_cf(cf_johari, &serialize_uuid(&node.id))?.is_some();
assert!(exists, "Node MUST exist in johari CF after store");
```

### 3. Boundary & Edge Case Audit

**Edge Case 1: Empty Database**
- BEFORE: Database has 0 nodes
- ACTION: `get_nodes_by_quadrant(Open, None, 0)`
- AFTER: Returns `Ok(Vec::new())` - NOT an error

**Edge Case 2: Maximum Pagination**
- BEFORE: 10 nodes exist with tag "test"
- ACTION: `get_nodes_by_tag("test", Some(100), 0)`
- AFTER: Returns all 10 nodes (limit > count is OK)

**Edge Case 3: Invalid Time Range**
- BEFORE: Nodes exist with timestamps
- ACTION: `get_nodes_in_time_range(future_start, future_end, None, 0)`
- AFTER: Returns empty Vec (no nodes in range)

### 4. Evidence of Success

Print statements in tests show:
```
=== TEST: get_nodes_by_quadrant finds stored nodes ===
RESULT: Open=2, Hidden=1
```

## Constraints (MUST Follow)

1. **NO async** - RocksDbMemex methods are synchronous (not async)
2. **Results are Vec<NodeId>** - NOT full MemoryNode objects
3. **Empty results = empty Vec** - NOT an error
4. **Pagination during iteration** - NOT post-filtering (efficiency)
5. **Prefix scans terminate** - Break when prefix no longer matches
6. **NO mocks in tests** - Use real store_node() then query

## Test Helper Reuse

The test file `tests_node.rs` already exports helpers. Reuse them:
```rust
use super::tests_node::{create_temp_db, create_valid_test_node, create_node_with_tags};
```

Note: These are `pub(crate)` so they're accessible within the crate tests.

## Final Verification Step

After completing implementation, you MUST spawn a `sherlock-holmes` subagent to verify:
1. All tests pass
2. Zero clippy warnings
3. Index queries return correct data
4. No regressions in existing node/edge tests
5. All edge cases properly handled

---

*Task ID: TASK-M02-023*
*Module: 02 - Core Infrastructure*
*Layer: Surface*
*Last Updated: 2025-12-31 - Corrected file paths and added verification requirements*
