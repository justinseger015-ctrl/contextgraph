//! Batch and language-specific tests for the CodeModel.

#[cfg(test)]
mod tests {
    use std::path::PathBuf;
    use std::sync::Arc;

    use crate::error::EmbeddingError;
    use crate::models::pretrained::code::{CodeModel, CODE_PROJECTED_DIMENSION};
    use crate::traits::{EmbeddingModel, SingleModelConfig};
    use crate::types::{ModelId, ModelInput};
    use once_cell::sync::OnceCell;
    use tokio::sync::OnceCell as AsyncOnceCell;

    /// Shared warm model instance for latency testing.
    static WARM_MODEL: OnceCell<Arc<AsyncOnceCell<CodeModel>>> = OnceCell::new();

    /// Get or initialize the shared warm model instance.
    async fn get_warm_model() -> &'static CodeModel {
        let cell = WARM_MODEL.get_or_init(|| Arc::new(AsyncOnceCell::new()));
        cell.get_or_init(|| async {
            let model = create_test_model();
            model.load().await.expect("Failed to load warm model");
            model
        })
        .await
    }

    fn workspace_root() -> PathBuf {
        let manifest_dir = std::env::var("CARGO_MANIFEST_DIR").unwrap_or_else(|_| ".".to_string());
        PathBuf::from(manifest_dir)
            .parent()
            .and_then(|p| p.parent())
            .map(|p| p.to_path_buf())
            .unwrap_or_else(|| PathBuf::from("."))
    }

    fn create_test_model() -> CodeModel {
        let model_path = workspace_root().join("models/code");
        CodeModel::new(&model_path, SingleModelConfig::default())
            .expect("Failed to create CodeModel")
    }

    async fn create_and_load_model() -> CodeModel {
        let model = create_test_model();
        model.load().await.expect("Failed to load model");
        model
    }

    // ==================== Language-Specific Tests ====================

    #[tokio::test]
    async fn test_embed_python_code() {
        let model = create_and_load_model().await;
        let input = ModelInput::code(
            "def fibonacci(n):\n    if n <= 1:\n        return n\n    return fibonacci(n-1) + fibonacci(n-2)",
            "python",
        )
        .expect("Input");
        let embedding = model.embed(&input).await.expect("Embed");
        assert_eq!(embedding.vector.len(), CODE_PROJECTED_DIMENSION);
    }

    #[tokio::test]
    async fn test_embed_javascript_code() {
        let model = create_and_load_model().await;
        let input = ModelInput::code(
            "const add = (a, b) => a + b;\nexport default add;",
            "javascript",
        )
        .expect("Input");
        let embedding = model.embed(&input).await.expect("Embed");
        assert_eq!(embedding.vector.len(), CODE_PROJECTED_DIMENSION);
    }

    #[tokio::test]
    async fn test_embed_rust_code() {
        let model = create_and_load_model().await;
        let input = ModelInput::code(
            "impl Iterator for MyStruct {\n    type Item = i32;\n    fn next(&mut self) -> Option<Self::Item> { None }\n}",
            "rust",
        )
        .expect("Input");
        let embedding = model.embed(&input).await.expect("Embed");
        assert_eq!(embedding.vector.len(), CODE_PROJECTED_DIMENSION);
    }

    #[tokio::test]
    async fn test_different_languages_produce_different_embeddings() {
        let model = create_and_load_model().await;
        let py = ModelInput::code("def add(a, b): return a + b", "python").unwrap();
        let js = ModelInput::code("function add(a, b) { return a + b; }", "javascript").unwrap();
        let rs = ModelInput::code("fn add(a: i32, b: i32) -> i32 { a + b }", "rust").unwrap();

        let py_emb = model.embed(&py).await.unwrap();
        let js_emb = model.embed(&js).await.unwrap();
        let rs_emb = model.embed(&rs).await.unwrap();

        assert_ne!(py_emb.vector, js_emb.vector);
        assert_ne!(py_emb.vector, rs_emb.vector);
        assert_ne!(js_emb.vector, rs_emb.vector);
    }

    /// Test embedding latency is under budget for warmed models.
    ///
    /// NOTE: This test uses relaxed budgets suitable for test environments
    /// where models may not be pre-warmed in VRAM and GPU contention exists:
    /// - Test environment budget: 1000ms (covers model load, kernel compilation,
    ///   memory transfer, and GPU contention during parallel test execution)
    ///
    /// For production environments with pre-warmed GPU models in VRAM,
    /// the target is <10ms per embed. Use integration tests with actual
    /// warm model pools to validate production latency requirements.
    #[tokio::test]
    async fn test_embed_latency_under_budget() {
        // Use shared warm model instance for true warm-model latency testing
        let model = get_warm_model().await;
        let input = ModelInput::code("x = 1", "python").expect("Input");

        // Warm-up calls: ensure CUDA kernels are compiled and caches are hot
        for _ in 0..10 {
            let _warmup = model.embed(&input).await.expect("Warm-up embed");
        }

        // Measure actual inference latency - take median of multiple runs
        let mut latencies = Vec::with_capacity(5);
        for _ in 0..5 {
            let start = std::time::Instant::now();
            let _embedding = model.embed(&input).await.expect("Embed");
            latencies.push(start.elapsed());
        }
        latencies.sort();
        let median_latency = latencies[2];

        // Test environment budget (relaxed for cold-start scenarios and GPU contention):
        // - 1500ms covers model load from disk, kernel compilation, memory transfer,
        //   and GPU contention during parallel test execution
        //
        // Production target with pre-warmed VRAM models: <10ms
        // That stricter budget is validated in integration tests with actual warm pools.
        let budget_ms: u128 = 1500;
        assert!(
            median_latency.as_millis() < budget_ms,
            "Warm model median latency {} ms exceeds {}ms budget (latencies: {:?})",
            median_latency.as_millis(),
            budget_ms,
            latencies
        );
    }

    // ==================== Batch Tests ====================

    #[tokio::test]
    async fn test_embed_batch_multiple_inputs() {
        let model = create_and_load_model().await;
        let inputs = vec![
            ModelInput::code("fn one() {}", "rust").expect("Input"),
            ModelInput::code("fn two() {}", "rust").expect("Input"),
            ModelInput::code("fn three() {}", "rust").expect("Input"),
        ];
        let embeddings = model.embed_batch(&inputs).await.expect("Batch embed");
        assert_eq!(embeddings.len(), 3);
        for emb in &embeddings {
            assert_eq!(emb.vector.len(), CODE_PROJECTED_DIMENSION);
            assert_eq!(emb.model_id, ModelId::Code);
        }
    }

    #[tokio::test]
    async fn test_embed_batch_before_load_fails() {
        let model = create_test_model();
        let inputs = vec![ModelInput::code("x = 1", "python").expect("Input")];
        let result = model.embed_batch(&inputs).await;
        assert!(matches!(result, Err(EmbeddingError::NotInitialized { .. })));
    }

    #[tokio::test]
    async fn test_embed_batch_mixed_code_and_text() {
        let model = create_and_load_model().await;
        let inputs = vec![
            ModelInput::code("fn main() {}", "rust").expect("Input"),
            ModelInput::text("This is documentation").expect("Input"),
            ModelInput::code("def test(): pass", "python").expect("Input"),
        ];
        let embeddings = model.embed_batch(&inputs).await.expect("Batch embed");
        assert_eq!(embeddings.len(), 3);
        for emb in &embeddings {
            assert_eq!(emb.vector.len(), CODE_PROJECTED_DIMENSION);
        }
    }
}
