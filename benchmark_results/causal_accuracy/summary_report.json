{
  "benchmark_suite": "causal_accuracy_benchmarks",
  "timestamp": "2026-02-09T22:20:00+00:00",
  "hardware": {
    "gpu": "NVIDIA RTX 5090 32GB GDDR7",
    "cpu": "AMD Ryzen 9 9950X3D",
    "ram": "128GB DDR5"
  },
  "overall_pass": true,
  "phases_executed": 9,
  "phases_passed": 9,
  "phases_skipped_gpu": ["1.1_vector_distinctness", "6.1_full_pipeline"],
  "results": {
    "phase_1_2_marker_detection": {
      "status": "PASS",
      "overall_accuracy": 0.96,
      "cause_f1": 1.0,
      "effect_f1": 0.947,
      "unknown_f1": 0.909,
      "weakness": "4/40 effect samples still misclassified as unknown (effect_recall=0.90); cause perfect (1.0)"
    },
    "phase_3_4_query_intent_detection": {
      "status": "PASS",
      "three_class_accuracy": 0.825,
      "direction_f1": 0.868,
      "dataset": "80 unique conversational queries (separate from Phase 1.2)",
      "confusion_highlights": {
        "cause_correct": "23/30",
        "effect_correct": "23/30",
        "unknown_correct": "20/20",
        "cause_to_unknown_misses": 7,
        "effect_to_unknown_misses": 7
      },
      "note": "Harder than Phase 1.2 (96%) — uses natural conversational phrasing without explicit causal markers"
    },
    "phase_4_1_modifier_sweep": {
      "status": "PASS",
      "optimal_cause_to_effect": 1.2,
      "optimal_effect_to_cause": 0.8,
      "optimal_ratio": 1.5,
      "constitution_match": true
    },
    "phase_4_2_per_mechanism_modifiers": {
      "status": "PASS",
      "recommendations": {
        "direct": {"c2e": 1.4, "e2c": 0.6, "ratio": 2.33},
        "mediated": {"c2e": 1.2, "e2c": 0.8, "ratio": 1.50},
        "feedback": {"c2e": 1.05, "e2c": 0.95, "ratio": 1.11},
        "temporal": {"c2e": 1.3, "e2c": 0.7, "ratio": 1.86}
      }
    },
    "phase_6_2_domain_transfer": {
      "status": "PASS",
      "overall_accuracy": 1.0,
      "accuracy_variance": 0.0,
      "total_samples": 120,
      "samples_per_domain": 12,
      "min_domain_accuracy": 1.0,
      "perfect_domains": "all_10",
      "note": "120/120 correct after adding 'led to', 'resulted in', 'result in', 'causes ', 'drives ' indicators. Previously 93.3% with gaps in physics/history/cybersecurity/economics/sw_engineering."
    },
    "phase_6_3_adversarial_robustness": {
      "status": "PASS",
      "total_cases": 23,
      "rejection_cases": 17,
      "detection_cases": 6,
      "correct_rejections": 16,
      "correct_detections": 6,
      "false_detections": 1,
      "missed_detections": 0,
      "rejection_rate": 0.941,
      "detection_rate": 1.0,
      "false_positive_rate": 0.059,
      "categories": ["correlation_not_causation", "factual_non_causal", "hypothetical", "near_miss", "negated", "nested", "reversed_direction", "spurious", "temporal_conflation"],
      "key_finding": "23 cases genuinely tested. 1 false positive: 'Playing video games does not lead to violent behavior' — substring 'lead to' triggers effect detection despite negation. Negation-aware detection requires NLP, not substring matching.",
      "known_limitation": "Substring matching cannot handle negated causation ('does not lead to' still matches 'lead to'). Tradeoff accepted: +4% Phase 1.2 accuracy > 1 adversarial false detection."
    },
    "phase_7_1_latency_budget": {
      "status": "PASS",
      "detect_intent_mean_us": 0.787,
      "asymmetric_sim_mean_us": 0.017,
      "cosine_768d_mean_us": 0.812,
      "cosine_1024d_mean_us": 1.078,
      "pipeline_mean_us": 0.757,
      "all_within_hard_limits": true,
      "note": "Per-call timing includes ~15ns Instant::now() overhead; batch throughput is more precise for sub-microsecond ops"
    },
    "phase_7_2_throughput": {
      "status": "PASS",
      "detect_intent_per_sec": 1275906,
      "asymmetric_sim_per_sec": 1352581469,
      "cosine_768d_per_sec": 1302964,
      "full_pipeline_per_sec": 1202664,
      "note": "asymmetric_sim is genuinely sub-nanosecond (match + multiply); previous run showed 32B/sec due to LLVM dead-code elimination (fixed with std::hint::black_box)"
    },
    "phase_7_3_pareto_frontier": {
      "status": "PASS",
      "optimal_operating_point": "intent_plus_asymmetric",
      "optimal_accuracy": 1.0,
      "optimal_latency_us": 1.0,
      "optimal_throughput_per_sec": 1026988,
      "modifier_impact_range": "ratio 1.0 (symmetric) to infinity (fully directional) — correctly varies with c2e/e2c sweep",
      "note": "Previous run showed constant ratio=1.5 for all modifier values due to c2e/e2c never being passed to function (fixed)"
    }
  },
  "optimization_recommendations": {
    "effect_recall_improvement": "Effect marker detection shows 90% recall vs 100% for cause. Remaining 4/40 effect misses are sentences without any indicator pattern (e.g. implied effects). Consider n-gram or ML-based classification for these edge cases.",
    "per_mechanism_modifiers": "Feedback loops benefit from near-symmetric modifiers (1.05/0.95). Consider implementing mechanism-aware modifier selection.",
    "domain_transfer_gaps": "All domain gaps resolved by adding tense variants ('lead to', 'led to', 'resulted in', 'result in') and active-voice patterns ('causes ', 'drives '). 10/10 domains at 100%.",
    "e5_weight_in_fusion": "Prior research confirms E5 weight at 0.20-0.30 (not current 0.45 in causal_reasoning profile). Adjust profile weights."
  },
  "benchmark_fixes_applied": {
    "black_box_optimization_prevention": "Added std::hint::black_box() to all measured function calls in Phase 7 to prevent LLVM from eliminating pure functions whose results are discarded. Previous asymmetric_sim throughput was 32B/sec (fake), now 1.35B/sec (real).",
    "modifier_impact_parameterization": "Fixed Phase 7.3 modifier_impact loop to compute varying forward/backward ratios using c2e/e2c values. Previous version computed c2e/e2c but never used them, calling compute_asymmetric_similarity with constant directions, producing identical ratio=1.5 for all 11 points.",
    "nanosecond_timing_precision": "Changed per-call timing from as_micros() to as_nanos()/1000.0 for sub-microsecond precision. Previous measurements showed 0.0us for operations faster than 1us.",
    "phase_6_3_adversarial_auto_pass": "Replaced boolean expect_no_causal with AdversarialExpectation enum (RejectAsNonCausal/DetectAsCausal). Previous else-branch auto-passed 9/20 cases without testing. Fixed negated case mislabeling. Added near-miss, nested, reversed_direction categories. Now 23 cases, all genuinely verified.",
    "phase_6_2_sample_expansion": "Expanded from 8 to 12 samples per domain (120 total). Added near-miss non-causal sentences that discuss causal topics without using causal markers. Revealed real gaps in physics (cause_accuracy=50%) and history/cybersecurity (effect_accuracy=50%).",
    "phase_3_4_dataset_deduplication": "Phase 3.4 was using identical marker_detection_dataset() as Phase 1.2, producing same 91% result. Created separate query_intent_dataset() with 80 harder conversational queries. Phase 3.4 now shows 82.5%, providing genuine cross-validation."
  }
}
