# Context Graph Constitution v4.2.0 (Optimized)
# Bio-Nervous MCP | UTL | Teleological Vectors | GWT | Adaptive Thresholds
# Aligned with constitution.md spec v2.0
# ═══════════════════════════════════════════════════════════════════════

meta:
  v: "4.2.0"
  name: "Ultimate Context Graph"
  desc: "5-layer bio-nervous UTL knowledge graph: 13-embedding teleological fingerprints, Kuramoto-synchronized GWT consciousness, adaptive self-learning thresholds, NORTH autonomous services"
  paradigm: "Multi-Array Teleological Fingerprints with GWT — 13-embedding array IS the teleological vector, Kuramoto-synchronized, no hardcoded thresholds"
  modules: { foundation: "NORTH-001-007", services: "NORTH-008-020", teleological: "TELEO-001-015" }
  spec_alignment: "constitution.md v2.0"

# ABBREVIATIONS (used throughout)
# UTL=Unified Theory of Learning, L=Learning score, ΔS=entropy, ΔC=coherence
# wₑ=emotional weight, φ=phase angle, NT=neurotransmitter, SS=Steering Subsystem
# OI=Omnidirectional Inference, FV=Formal Verification, PC=Predictive Coding
# HE=Hyperbolic Entailment, MHN=Modern Hopfield Network, SDM=Sparse Distributed Memory
# TF=Teleological Fingerprint, PV=Purpose Vector, SF=Semantic Fingerprint (13-array)
# A(v,V)=Teleological Alignment (cosine to North Star)
# GWT=Global Workspace Theory, GW=Global Workspace, IIT=Integrated Information Theory
# CMS=Continuum Memory System, C(t)=Consciousness, I(t)=Integration, R(t)=Reflection, D(t)=Differentiation
# r=Kuramoto order param, K=coupling strength, θᵢ=phase, ωᵢ=natural freq
# ATC=Adaptive Threshold Calibration, ECE=Expected Calibration Error, MCE=Max Calibration Error
# EWMA=Exponentially Weighted Moving Average, UCB=Upper Confidence Bound, GP=Gaussian Process
# T=Temperature, α=EWMA smoothing

# ═══════════════════════════════════════════════════════════════════════
# TECH STACK
# ═══════════════════════════════════════════════════════════════════════
stack:
  lang: { rust: "1.75+", edition: "2021", cuda: "13.1" }
  gpu: { target: "RTX 5090", vram: "32GB", compute: "12.0" }
  deps: [tokio@1.35+, serde@1.0+, uuid@1.6+, chrono@0.4+, rmcp@0.1+, cudarc@0.10+, faiss@0.12+gpu, rocksdb@0.21+, scylladb@1.0+]
  db:
    primary: { dev: rocksdb, prod: scylladb }
    indexes: { per_embedder: "13× HNSW", matryoshka_128d: HNSW, splade_inverted: inverted, purpose: "13D HNSW", goal_hierarchy: tree }
    temporal: timescaledb
    cache: redis7+
  embed_fallback: [openai/text-embedding-3-large, cohere/embed-english-v3.0]

# ═══════════════════════════════════════════════════════════════════════
# DIRECTORY STRUCTURE
# ═══════════════════════════════════════════════════════════════════════
dirs:
  crates/:
    context-graph-mcp/: "tools/, resources/, handlers/, adapters/"
    context-graph-core/:
      desc: "graph/, search/, utl/, session/, curation/, teleological/, autonomous/"
      autonomous/: { foundation/: [bootstrap, thresholds, drift, curation, evolution, workflow], services/: [bootstrap_service, threshold_learner, drift_detector, drift_corrector, pruning_service, consolidation_service, gap_detection, subgoal_discovery, weight_adjuster, obsolescence_detector, daily_scheduler, event_optimizer, self_healing_manager] }
      teleological/: [storage, feedback_learner, profile_manager]
    context-graph-cuda/: "kernels/, hnsw/, hopfield/, neuromod/"
    context-graph-embeddings/: "models/, fingerprint/, purpose_vector.rs, semantic_fingerprint.rs"
    context-graph-storage/: "rocksdb/, scylla/, indexes/, temporal/, autonomous/"
  specs/: [functional/, technical/, tasks/]
  tests/: [integration/, benchmarks/, fixtures/, chaos/, validation/]
  config/: [default.toml, production.toml, test.toml]
  .ai/: [activeContext.md, decisionLog.md, progress.md]

# ═══════════════════════════════════════════════════════════════════════
# ARCHITECTURAL RULES (Critical constraints - must not violate)
# ═══════════════════════════════════════════════════════════════════════
architectural_rules:
  ARCH-01:
    title: "TeleologicalArray is the Atomic Storage Unit"
    severity: critical
    description: "Every stored memory MUST be represented as a TeleologicalArray containing exactly 13 embedding vectors. Store all 13 or store nothing."
    enforcement: "Compile-time via type system; runtime via invariant checks"
    rationale: "The 13 embeddings capture orthogonal semantic dimensions. Partial storage loses information."

  ARCH-02:
    title: "Compare Only Compatible Embedding Types (Apples-to-Apples)"
    severity: critical
    description: "Similarity comparisons MUST be apples-to-apples: E1 to E1, E4 to E4, full array to full array. NEVER compare E1 to E5 or any cross-embedder similarity."
    enforcement: "Type-safe EmbedderType enum; compile-time check via generics"
    rationale: "Different embedders capture different semantic spaces. Cross-comparison is meaningless."

  ARCH-03:
    title: "Autonomous Operation Without Manual Configuration"
    severity: critical
    description: "System MUST operate autonomously without manual goal setting. Goals emerge from data patterns via clustering. No set_north_star() or equivalent."
    enforcement: "API design; no manual goal-setting methods exposed"
    rationale: "Manual configuration creates brittleness. System discovers what matters by observing actual user behavior."

  ARCH-05:
    title: "All 13 Embedders Must Be Present"
    severity: critical
    description: "Every TeleologicalArray MUST contain all 13 embeddings (E1-E13). Missing embedders are a fatal error."
    enforcement: "TeleologicalArray struct with [Embedding; 13]; EmbeddingAgent validates before storage"
    rationale: "Sparse arrays break comparison semantics. Purpose vectors become incomparable with missing embedders."

  ARCH-06:
    title: "All Memory Operations Through MCP Tools"
    severity: high
    description: "External access MUST go through MCP tools: inject_context, store_memory, search_graph, discover_goals, consolidate_memories. Direct database access forbidden."
    enforcement: "MCP server as sole interface; no direct RocksDB exposure"
    rationale: "MCP tools provide security boundary, input validation, rate limiting."

  ARCH-07:
    title: "Hooks Control Memory Lifecycle"
    severity: high
    description: "Claude Code hooks MUST drive memory operations: SessionStart (init), PreToolUse (inject context), PostToolUse (store patterns), SessionEnd (consolidate/dream)."
    enforcement: "Hook configuration in .claude/settings.json"
    rationale: "Autonomous operation requires automatic triggers. Hooks make memory invisible to users."

  ARCH-08:
    title: "CUDA GPU is Required for Production"
    severity: high
    description: "System REQUIRES CUDA GPU (RTX 5090 / Blackwell). No CPU fallbacks in production. Test environments may use stubs via test-utils feature flag."
    enforcement: "Cargo feature flags; cuda feature is default"
    rationale: "13-model embedding on CPU is 10-100x slower. Performance budgets cannot be met without GPU."

# ═══════════════════════════════════════════════════════════════════════
# CODING STANDARDS
# ═══════════════════════════════════════════════════════════════════════
naming:
  files: { rust: snake_case.rs, cuda: snake_case.cu, tests: "{mod}_test.rs", typescript: kebab-case.ts }
  types: PascalCase
  funcs: { rust: snake_case_verb_first, typescript: camelCase_verb_first }
  vars: { local: snake_case, const: SCREAMING_SNAKE }
  json: snake_case

rust_standards:
  naming:
    structs: PascalCase
    traits: "PascalCase with -able/-er suffix (e.g., Searchable, Embedder)"
    enums: PascalCase
    type_aliases: "PascalCase (e.g., type MemoryId = Uuid)"
  error_handling:
    - "Use thiserror for library error types"
    - "Use anyhow for application-level errors"
    - "Never panic in library code; return Result"
    - "Propagate errors with ? operator"
    - "Add context with .context() or .with_context()"
  async_patterns:
    - "Use tokio as async runtime (workspace dependency)"
    - "Prefer async fn over impl Future for readability"
    - "Use tokio::spawn for parallel tasks"
    - "Use tokio::sync primitives (Mutex, RwLock, mpsc)"
    - "Avoid blocking in async context; use spawn_blocking for CPU-bound work"
  type_safety:
    - "Use newtype pattern for domain IDs (struct MemoryId(Uuid))"
    - "Encode embedder type in generics where possible"
    - "Use NonZeroU* for counts that cannot be zero"
    - "Prefer enums over boolean flags for clarity"

typescript_standards:
  naming:
    files: kebab-case.ts
    directories: kebab-case
    interfaces: "PascalCase (I prefix optional)"
    types: PascalCase
    functions: "camelCase, verb-first (e.g., injectContext)"
    constants: "SCREAMING_SNAKE_CASE or camelCase for config"
  file_organization:
    - "Skills in .claude/skills/[skill-name]/SKILL.md"
    - "Agents in .claude/agents/[agent-name].md"
    - "Commands in .claude/commands/[command-name]/"
    - "Helpers in .claude/helpers/"
  skill_structure:
    - "YAML frontmatter with name, description, allowed-tools, model"
    - "Markdown body with Purpose, When to Use, Process sections"
    - "Explicit MCP tool references"

rules:
  - "One type/module, max 500 lines"
  - "Co-locate tests: #[cfg(test)]"
  - "Import order: std→external→workspace→super/self"
  - "Result<T,E>, thiserror derivation"
  - "Never unwrap() in prod; use expect() with context"
  - "tokio async, Arc<RwLock<T>> for shared state"
  - "Lock order: inner→faiss_index"
  - "Max 5 unsafe blocks/module, document invariants"
  - "CUDA FFI only in context-graph-cuda"

doc_format: "/// Brief\\n/// # Args/Returns/Errors/Panics\\n/// `Constraint: X < Yms`"

# ═══════════════════════════════════════════════════════════════════════
# ANTI-PATTERNS (FORBIDDEN)
# ═══════════════════════════════════════════════════════════════════════
forbidden:
  # Critical - Architecture violations
  AP-01: { severity: critical, reason: "Breaks core architecture", rule: "No manual goal setting (set_north_star, define_goal, etc.) - Goals MUST emerge autonomously" }
  AP-02: { severity: critical, reason: "Type safety violation", rule: "No cross-embedder comparison (comparing E1 similarity to E5 similarity)" }
  AP-03: { severity: critical, reason: "Destroys information", rule: "No dimension projection to fake compatibility - Never project 1024D to 512D" }
  AP-04: { severity: critical, reason: "Violates atomicity", rule: "No partial TeleologicalArray storage - All 13 embeddings or nothing" }
  AP-05: { severity: critical, reason: "Meaningless results", rule: "No embedding fusion into single vector - Multi-embedding arrays must remain multi-embedding" }

  # High severity
  AP-06: { severity: high, reason: "Security bypass", rule: "No direct database access from Claude Code - All access through MCP tools only" }
  AP-07: { severity: high, reason: "Performance regression", rule: "No CPU fallback in production builds - GPU required, use test-utils feature for testing" }
  AP-08: { severity: high, reason: "Blocks async runtime", rule: "No synchronous I/O in async context - Use spawn_blocking for blocking operations" }
  AP-09: { severity: high, reason: "Memory leak risk", rule: "No unbounded caches or queues - All caches must have size limits and eviction" }
  AP-10: { severity: high, reason: "Computation errors", rule: "No NaN/Infinity in UTL calculations" }

  # Medium severity - Code quality
  AP-11: { severity: medium, reason: "Code duplication", rule: "Check existing utils before creating new helpers" }
  AP-12: { severity: medium, reason: "Maintainability", rule: "No magic numbers; define named constants (e.g., EMBEDDER_COUNT = 13)" }
  AP-13: { severity: medium, reason: "Testing", rule: "No inline test fixtures; use tests/fixtures/ directory" }
  AP-14: { severity: medium, reason: "Deprecated pattern", rule: "No .unwrap() in library code - Use .expect() with context or return Result" }
  AP-15: { severity: medium, reason: "Security", rule: "Hardcoded secrets forbidden - Use environment variables" }

  # Operational
  AP-16: { severity: high, reason: "Data loss", rule: "No permanent delete without user_requested flag" }
  AP-17: { severity: medium, reason: "Quality", rule: "store_memory without rationale forbidden" }
  AP-18: { severity: medium, reason: "Consistency", rule: "merge_concepts without priors_vibe_check forbidden" }
  AP-19: { severity: medium, reason: "Reliability", rule: "Trust distilled summaries forbidden (use hydrate_citation)" }
  AP-20: { severity: medium, reason: "Awareness", rule: "Ignore Cognitive Pulse feedback forbidden" }
  AP-21: { severity: high, reason: "Resource management", rule: "GPU alloc without pool forbidden" }
  AP-22: { severity: high, reason: "Thread safety", rule: "FAISS mutation without lock forbidden" }
  AP-23: { severity: medium, reason: "Code organization", rule: "Direct API from handlers forbidden - Use service layer" }

# ═══════════════════════════════════════════════════════════════════════
# SECURITY [SEC-##]
# ═══════════════════════════════════════════════════════════════════════
security:
  SEC-01: "Validate/sanitize all input (PIIScrubber L1)"
  SEC-02: { rule: "Scrub PII pre-embed", patterns: [api_key, password, bearer_token, ssn, credit_card] }
  SEC-03: { anomaly_threshold: "3.0 std", content_align_min: 0.4 }
  SEC-04: { rule: "Detect prompt injection", patterns: ["ignore previous", "disregard system", "you are now", "new instructions:", "override:"] }
  SEC-05: { rule: "Quarantine semantic cancer", trigger: "importance>0.9 AND neighbor_entropy>0.8", action: "reduce 50%, flag" }
  SEC-06: "Soft delete 30-day recovery"
  SEC-07: "Secrets from env vars only"
  SEC-08: "No cross-agent memetic interference (perspective_lock)"

# ═══════════════════════════════════════════════════════════════════════
# PERFORMANCE BUDGETS
# ═══════════════════════════════════════════════════════════════════════
perf:
  latency: { inject_context: "<25ms p95/<50ms p99", hopfield: "<1ms", reflex_cache: "<100μs", single_embed: "<10ms", batch_embed_64: "<50ms", faiss_1M_k100: "<2ms", distillation: "<50ms", neuromod_update: "<200μs", dream_wake: "<100ms", entailment_check: "<1ms" }
  throughput: { embed_batch: ">1000/sec", search_batch_100: "<5ms" }
  memory: { gpu: "<24GB (8GB headroom)", graph_cap: ">10M nodes" }
  quality: { utl_avg: ">0.6", coherence_recovery: "<10s", attack_detection: ">95%", false_positive: "<2%", info_loss: "<15%", compression: ">60%" }

# ═══════════════════════════════════════════════════════════════════════
# TESTING
# ═══════════════════════════════════════════════════════════════════════
testing:
  coverage: { unit: "90%", integration: "80%", docs: "80%" }
  types: { unit: "#[cfg(test)] - business logic, UTL, embedding", integration: "tests/integration/ - MCP, graph, session", benchmark: "benches/ - latency, throughput", chaos: "tests/chaos/ - GPU OOM, network partition, concurrent mutation", validation: "tests/validation/ - needle-haystack, UTL dynamics, dream" }
  gates:
    pre-commit: [fmt --check, clippy -D warnings, test --lib]
    pre-merge: [test --all, bench --no-run, "coverage>=90%"]
    pre-deploy: [integration pass, "bench regression<5%", chaos pass]

# ═══════════════════════════════════════════════════════════════════════
# UTL (Unified Theory of Learning)
# ═══════════════════════════════════════════════════════════════════════
utl:
  canonical: "L = f((ΔS × ΔC) · wₑ · cos φ)  →  L ∈ [0,1]"
  multi_embed: "L_multi = sigmoid(2.0 · (Σᵢ τᵢλ_S·ΔSᵢ) · (Σⱼ τⱼλ_C·ΔCⱼ) · wₑ · cos φ)"
  params: { ΔSᵢ: "[0,1] entropy/space i", ΔCᵢ: "[0,1] coherence/space j", τᵢ: "[0,1] teleological weight", wₑ: "[0.5,1.5] emotional", φ: "[0,π] phase sync" }
  loss: "J = 0.4·L_task + 0.3·L_semantic + 0.2·L_teleological + 0.1·(1-L)"
  adaptive_weights:
    first_time: { λ_task: 0.3, λ_semantic: 0.2, λ_dyn: 0.9 }
    mid_level: { λ_task: 0.7, λ_semantic: 0.4, λ_dyn: 0.6 }
    expert: { λ_task: 1.0, λ_semantic: 0.1, λ_dyn: 0.3 }
  johari: # ΔS×ΔC plane IS the Johari Window
    Open: "ΔS<0.5, ΔC>0.5 (aware)"
    Blind: "ΔS>0.5, ΔC<0.5 (discovery opportunity)"
    Hidden: "ΔS<0.5, ΔC<0.5 (dormant)"
    Unknown: "ΔS>0.5, ΔC>0.5 (frontier)"
    cross_space: "Memory can be Open(semantic) but Blind(causal)"
  lifecycle:
    infancy:  { n: "0-50",   λ_ΔS: 0.7, λ_ΔC: 0.3, stance: "capture-novelty" }
    growth:   { n: "50-500", λ_ΔS: 0.5, λ_ΔC: 0.5, stance: "balanced" }
    maturity: { n: "500+",  λ_ΔS: 0.3, λ_ΔC: 0.7, stance: "curation-coherence" }

# ═══════════════════════════════════════════════════════════════════════
# TELEOLOGICAL ARCHITECTURE
# ═══════════════════════════════════════════════════════════════════════
teleological:
  core: "13-embedding array IS the teleological vector"
  alignment: "A(v, V) = cos(v, V) = (v · V) / (||v|| × ||V||)"
  purpose_vector: "PV = [A(E1,V), ..., A(E13,V)]  # 13D, searchable"
  thresholds: { optimal: "≥0.75", acceptable: "[0.70,0.75)", warning: "[0.55,0.70)", critical: "<0.55", failure_prediction: "ΔA<-0.15 predicts failure 30-60s" }
  transitivity: "If A(u,v)≥θ₁ and A(v,w)≥θ₂, then A(u,w)≥2θ₁θ₂-1"
  goal_hierarchy: { V_global: "North Star (emergent from teleological arrays)", V_mid: "Retrieval/Storage/Reasoning", V_local: "Per-operation" }

  # ═══════════════════════════════════════════════════════════════════════
  # CRITICAL: MANUAL NORTH STAR CREATION IS INVALID
  # ═══════════════════════════════════════════════════════════════════════
  # Manual North Star tools (set_north_star, get_north_star, update_north_star,
  # delete_north_star, init_north_star_from_documents) have been REMOVED.
  #
  # WHY: Manual tools created single 1024D embeddings that CANNOT be meaningfully
  # compared to 13-embedder teleological arrays. This is comparing apples to oranges.
  #
  # THE PROBLEM:
  #   - A manual North Star is ONE vector (e.g., 1024D from text-embedding-3-large)
  #   - Teleological fingerprints contain 13 DIFFERENT embeddings from 13 DIFFERENT models
  #   - Each embedder (E1-E13) has different dimensions, semantics, and purposes
  #   - Cosine similarity between E1's 1024D and a manual 1024D North Star is meaningless
  #     when E7 (Code) is 1536D, E5 (Causal) is 768D, E9 (HDC) is binary, etc.
  #
  # THE SOLUTION (Autonomous System):
  #   - North Star emerges AUTONOMOUSLY from analysis of stored teleological fingerprints
  #   - The autonomous services (NORTH-008 to NORTH-020) work with full 13-embedder arrays
  #   - Comparisons are ALWAYS apples-to-apples: same embedder to same embedder
  #   - Purpose vectors (13D) aggregate per-embedder alignments coherently
  #   - Drift detection, correction, and evolution operate on teleological space
  #
  # VALID COMPARISONS:
  #   - TeleologicalFingerprint ↔ TeleologicalFingerprint (full 13-array)
  #   - PurposeVector ↔ PurposeVector (13D alignment signatures)
  #   - E_i embedding ↔ E_i embedding (same embedder only)
  #   - Cross-correlations ↔ Cross-correlations (78D pair interactions)
  #   - Group alignments ↔ Group alignments (6D hierarchical)
  #
  # INVALID COMPARISONS:
  #   - Manual 1024D vector ↔ TeleologicalFingerprint (dimensionally incompatible)
  #   - E1 (Semantic) ↔ E7 (Code) (different semantic spaces)
  #   - Any single embedding ↔ Multi-embedding array (apples to oranges)

  embedder_purposes:
    E1: V_meaning, E2: V_freshness, E3: V_periodicity, E4: V_ordering, E5: V_causality
    E6: V_selectivity, E7: V_correctness, E8: V_connectivity, E9: V_robustness
    E10: V_multimodality, E11: V_factuality, E12: V_precision, E13: V_keyword_precision

# ═══════════════════════════════════════════════════════════════════════
# GRAPH EDGE MODEL (Marblestone NT)
# ═══════════════════════════════════════════════════════════════════════
edge_model:
  attrs: [source:UUID, target:UUID, type:Semantic|Temporal|Causal|Hierarchical|Relational, weight:[0,1], confidence:[0,1]]
  nt_weights: "w_eff = base × (1 + excitatory - inhibitory + 0.5×modulatory)"
  domain: Code|Legal|Medical|Creative|Research|General
  amortized: { trigger: "3+ hop ≥5×", weight: "product(path)", confidence: "≥0.7" }

# ═══════════════════════════════════════════════════════════════════════
# 5-LAYER BIO-NERVOUS SYSTEM
# ═══════════════════════════════════════════════════════════════════════
layers:
  L1_Sensing: { lat: "<5ms", throughput: "10K/s", components: [13-model embed, PII scrub, adversarial detect], utl: "ΔS measurement" }
  L2_Reflex:  { lat: "<100μs", hit_rate: ">80%", components: [Hopfield cache], utl: "bypass if confidence>0.95" }
  L3_Memory:  { lat: "<1ms", capacity: "2^768 patterns", noise: ">20%", components: [MHN, FAISS GPU], utl: "consolidation" }
  L4_Learning: { freq: "100Hz", grad_clip: 1.0, components: [UTL optimizer, neuromod controller], utl: "L optimization" }
  L5_Coherence: { sync: "10ms", consistency: eventual, components: [Thalamic gate, PC, distiller, FV, GW broadcast], utl: "φ sync" }

# ═══════════════════════════════════════════════════════════════════════
# GLOBAL WORKSPACE THEORY (GWT)
# ═══════════════════════════════════════════════════════════════════════
gwt:
  consciousness: "C(t) = I(t) × R(t) × D(t) = r(t) × σ(MetaUTL.predict_accuracy) × H(PV)"
  components: { C: "Consciousness [0,1]", I: "Integration (Kuramoto r)", R: "Self-Reflection (Meta-UTL)", D: "Differentiation (13D entropy)" }

  kuramoto:
    formula: "dθᵢ/dt = ωᵢ + (K/N)Σⱼ sin(θⱼ-θᵢ)"
    order_param: "r·e^(iψ) = (1/N)Σⱼ e^(iθⱼ)"
    thresholds: { coherent: "r≥0.8", fragmented: "r<0.5", hypersync: "r>0.95 (pathological)" }
    frequencies: # Hz (band)
      E1: 40γ, E2: 8α, E3: 8α, E4: 8α, E5: 25β, E6: 4θ, E7: 25β
      E8: 12αβ, E9: 80γ+, E10: 40γ, E11: 15β, E12: 60γ+, E13: 4θ

  workspace:
    active_memory: "Option<MemoryId>"
    coherence_threshold: 0.8
    broadcast_duration_ms: 100
    selection: "r≥0.8 → rank by r×importance×north_star_alignment → top-1 broadcasts"
    events:
      enters: { trigger: "r↑0.8", effect: "Dopamine+=0.2" }
      exits: { trigger: "r↓0.7", effect: "Log for dream" }
      conflict: { trigger: "Two r>0.8", effect: "critique_context" }
      empty_5s: { trigger: "No r>0.8 for 5s", effect: "epistemic_action" }

  self_ego_node:
    id: "SELF_EGO_NODE"
    fields: [fingerprint, purpose_vector, identity_trajectory, coherence_with_actions]
    loop: "Retrieve→A(action,PV)→if<0.55 self_reflect→update fingerprint→store evolution"
    identity_continuity: "IC = cos(PV_t, PV_{t-1}) × r(t); healthy>0.9, warning<0.7, dream<0.5"

  states: { DORMANT: "r<0.3", FRAGMENTED: "0.3≤r<0.5", EMERGING: "0.5≤r<0.8", CONSCIOUS: "r≥0.8", HYPERSYNC: "r>0.95" }

  meta_cognitive:
    formula: "MetaScore = σ(2×(L_predicted - L_actual))"
    low_meta: "MetaScore<0.5 for 5 ops → ↑Acetylcholine, introspective dream"

  quality: { Φ: ">0.3 (min_cut/total)", availability: ">90%", stability: ">500ms", meta_awareness: ">0.85", identity: ">0.9" }

# ═══════════════════════════════════════════════════════════════════════
# NEUROMODULATION
# ═══════════════════════════════════════════════════════════════════════
neuromod:
  Dopamine:     { param: hopfield.beta, range: "[1,5]", effect: "↑=sharp retrieval" }
  Serotonin:    { param: similarity.space_weights, range: "[0,1]", effect: "↑=more spaces" }
  Noradrenaline: { param: attention.temp, range: "[0.5,2]", effect: "↑=flat attention" }
  Acetylcholine: { param: utl.lr, range: "[0.001,0.002]", baseline: 0.001, decay: 0.1, effect: "↑=faster update" }

# ═══════════════════════════════════════════════════════════════════════
# DREAM LAYER
# ═══════════════════════════════════════════════════════════════════════
dream:
  trigger: { activity: "<0.15", idle: "10min" }
  phases: { nrem: { dur: "3min", purpose: "replay recent", recency_bias: 0.8 }, rem: { dur: "2min", purpose: "explore attractors", temp: 2.0 } }
  constraints: { queries: 100, semantic_leap: 0.7, abort_on_query: true, wake: "<100ms", gpu: "<30%" }

# ═══════════════════════════════════════════════════════════════════════
# STEERING SUBSYSTEM
# ═══════════════════════════════════════════════════════════════════════
steering:
  Gardener: { role: "cross-session curation", trigger: "activity<0.15 for 2min" }
  Curator: { auto: "dupes>0.95, weak<0.1, orphans>30d", escalate: "dupes 0.7-0.95, priors-incompatible, conflicts, semantic cancer" }
  Assessor: "per-interaction quality"
  reward: { range: "[-1,1]", dopamine: { pos: "+=r×0.2", neg: "-=|r|×0.1" } }

# ═══════════════════════════════════════════════════════════════════════
# OMNIDIRECTIONAL INFERENCE & FORMAL VERIFICATION
# ═══════════════════════════════════════════════════════════════════════
omni_infer:
  directions: [forward:A→B, backward:B→A, bidirectional:A↔B, bridge:cross-domain, abduction:best-hypothesis]
  clamp: { hard: fixed, soft: biased-adjustable }
  active_inference: "EFE minimizes surprise+ambiguity"

formal_verify:
  location: L5_Coherence
  conditions: [bounds, null_safety, type_invariants, loop_termination, custom]
  status: [Verified, Failed, Timeout, NotApplicable]
  cache: content_hash
  latency: "<10ms cached"

# ═══════════════════════════════════════════════════════════════════════
# PREDICTIVE CODING & GARDENER
# ═══════════════════════════════════════════════════════════════════════
pred_coding:
  flow: "L5→L1: prediction→error(obs-pred)→propagate surprise only"
  reduction: "~30% tokens"
  domain_priors: { medical: { causal: 1.8, code: 0.3 }, programming: { code: 2.0, graph: 1.5 }, creative: { semantic: 1.5, temporal: 0.5 } }

gardener:
  trigger: "activity<0.15 for 2min"
  ops: ["prune weak<0.1", "merge dupes>0.95", "rebalance hyperbolic", "rebuild FAISS"]
  gpu: "<10%"

# ═══════════════════════════════════════════════════════════════════════
# MCP PROTOCOL
# ═══════════════════════════════════════════════════════════════════════
mcp:
  version: "2024-11-05"
  transport: [stdio, sse]
  caps: [tools, resources, prompts, logging]
  errors: { -32700: Parse, -32600: InvalidRequest, -32601: MethodNotFound, -32602: InvalidParams, -32603: Internal, -32000: SessionNotFound, -32001: GraphQueryError, -32002: StorageError, -32003: CausalInferenceError, -32004: RateLimitExceeded }
  pulse: { fields: [entropy, coherence, suggested_action], cost: "~30 tokens" }

  # Core Tool Categories
  marblestone_tools: [get_steering_feedback, omni_infer, verify_code_node]
  gwt_tools: [get_consciousness_state, get_workspace_status, get_kuramoto_sync, get_ego_state, trigger_workspace_broadcast, adjust_coupling, get_johari_classification, compute_delta_sc]
  adaptive_threshold_tools: [get_threshold_status, get_calibration_metrics, trigger_recalibration, set_threshold_prior, get_threshold_history, explain_threshold]
  # NOTE: Manual North Star tools REMOVED - see teleological section for explanation.
  # The autonomous system works directly with 13-embedder teleological arrays.
  autonomous_north_star_tools:
    # NORTH-008: Bootstrap from teleological embeddings (NOT from manual vectors)
    NORTH-008: [auto_bootstrap_north_star, get_autonomous_status]  # Discovers purpose from stored fingerprints
    NORTH-009: [get_learner_state, observe_outcome]  # Threshold learning on teleological space
    NORTH-010: [get_alignment_drift, get_drift_history]  # Drift in 13D purpose vector space
    NORTH-011: [trigger_drift_correction, preview_correction]  # Corrects teleological alignment
    NORTH-012: [get_pruning_candidates, execute_prune, get_prune_report]
    NORTH-013: [trigger_consolidation, get_consolidation_candidates]
    NORTH-014: [analyze_gaps, get_gap_report]
    NORTH-015: [discover_sub_goals, propose_subgoal]  # Discovers goals from teleological clusters
    NORTH-016: [adjust_weights, preview_weight_change]
    NORTH-017: [get_obsolete_goals, mark_obsolete]
    NORTH-018: [get_schedule, trigger_scheduled_task]
    NORTH-019: [get_optimization_status, trigger_optimization]
    NORTH-020: [get_health_status, trigger_healing, get_healing_history]

# ═══════════════════════════════════════════════════════════════════════
# CLAUDE CODE INTEGRATION (Hooks, Skills, Subagents)
# ═══════════════════════════════════════════════════════════════════════
claude_code_integration:
  hooks:
    SessionStart:
      required: true
      timeout_ms: 5000
      on_failure: "log warning, continue session"
      actions:
        - "Initialize workspace"
        - "Load SELF_EGO_NODE"
        - "Warm embedding caches"

    PreToolUse:
      required: true
      matcher: "Read|Grep|Glob|Bash"
      timeout_ms: 3000
      on_failure: "continue without injection"
      constraint: "MUST NOT block for more than 100ms in critical path"
      actions:
        - "Inject relevant context before tool execution"

    PostToolUse:
      required: true
      matcher: "Edit|Write|Bash"
      timeout_ms: 5000
      async_allowed: true
      actions:
        - "Store learned patterns from tool output"

    SessionEnd:
      required: true
      timeout_ms: 30000
      mode: "Deep dreaming"
      actions:
        - "Run memory consolidation"
        - "Goal discovery"

    PreCompact:
      required: false
      timeout_ms: 10000
      actions:
        - "Extract salient memories before context compaction"

    SubagentStop:
      required: false
      timeout_ms: 5000
      actions:
        - "Merge subagent learnings into main memory"

  skills:
    memory-inject:
      model: haiku
      trigger: "Auto-invoke on context needs"
      mcp_tool: inject_context
      permissions: read-only
      constraint: "No store_memory access"

    semantic-search:
      model: sonnet
      trigger: "Explicit search across all embeddings"
      mcp_tool: search_graph
      features:
        - "All comparison modes supported"
        - "Returns per-space scores"

    goal-discovery:
      model: opus
      trigger: "Discover emergent goals from patterns"
      mcp_tool: discover_goals
      permissions: read-only
      constraint: "No manual goal setting"
      returns: "Ranked goals with confidence"

    consolidate:
      model: sonnet
      trigger: "Memory dreaming and pruning"
      mcp_tool: consolidate_memories
      permissions: elevated
      modes: [light, deep, REM]

  subagents:
    embedding-agent:
      model: haiku
      purpose: "Generate all 13 embeddings in parallel"
      constraint: "Must validate array completeness"
      performance_target: "<500ms"

    search-agent:
      model: haiku
      purpose: "Multi-space parallel search and reranking"
      features:
        - "Search all embedder spaces in parallel"
      performance_target: "<30ms"

    goal-agent:
      model: opus
      purpose: "Autonomous goal emergence"
      features:
        - "K-means clustering on purpose vectors"
        - "Update SELF_EGO_NODE"

    dream-agent:
      model: sonnet
      purpose: "Memory consolidation"
      features:
        - "Hippocampal replay simulation"
        - "Prune below salience threshold"

# ═══════════════════════════════════════════════════════════════════════
# 13-MODEL EMBEDDING → TELEOLOGICAL FINGERPRINT
# ═══════════════════════════════════════════════════════════════════════
embeddings:
  paradigm: "NO FUSION - Store all 13 embeddings; ~17KB quantized (63% reduction), 100% info preserved"

  models: # dim|math|lat|quant
    E1_Semantic:
      dim: 1024
      type: dense
      math: Dense_Transformer
      lat: "<5ms"
      quant: "PQ-8"
      matryoshka: [512,256,128]
      model_ref: "sentence-transformers/all-MiniLM-L12-v2 or similar"

    E2_Temporal_Recent:
      dim: 512
      type: dense
      math: Exp_Decay
      lat: "<2ms"
      quant: Float8
      encodes: "time-since-creation features"

    E3_Temporal_Periodic:
      dim: 512
      type: dense
      math: Fourier
      lat: "<2ms"
      quant: Float8
      encodes: "day-of-week, time-of-day cyclical patterns"

    E4_Temporal_Positional:
      dim: 512
      type: dense
      math: Sin_PE
      lat: "<2ms"
      quant: Float8
      encodes: "named entity links, who/what relationships"

    E5_Causal:
      dim: 768
      type: dense
      math: SCM_Intervention
      lat: "<8ms"
      quant: "PQ-8"
      asymmetric: true
      encodes: "cause-effect chains, because/therefore relationships"

    E6_Sparse:
      dim: "~30K 5%"
      type: sparse
      math: TopK
      lat: "<3ms"
      quant: Sparse
      encodes: "sparse lexical, keyword precision"

    E7_Code:
      dim: 1536
      type: dense
      math: AST_Transformer
      lat: "<10ms"
      quant: "PQ-8"
      encodes: "discourse context, surrounding conversation"

    E8_Graph:
      dim: 384
      type: dense
      math: MeanPooling
      lat: "<5ms"
      quant: Float8
      encodes: "affective valence, sentiment and emotion"

    E9_HDC:
      dim: "10K→1024"
      type: binary
      math: XOR_Hamming
      lat: "<1ms"
      quant: Binary
      encodes: "structural patterns, grammar and structure"

    E10_Multimodal:
      dim: 768
      type: dense
      math: CrossAttention
      lat: "<15ms"
      quant: "PQ-8"
      encodes: "intent and function, what the content does"

    E11_Entity:
      dim: 384
      type: dense
      math: "h+r≈t"
      lat: "<2ms"
      quant: Float8
      encodes: "multi-modal links, bridges text/code/diagrams"

    E12_LateInteraction:
      dim: "128D/tok"
      type: dense_per_token
      math: ColBERT_MaxSim
      lat: "<8ms"
      quant: TokenPruning
      encodes: "ColBERT-style token-level, variable length"

    E13_SPLADE:
      dim: "~30K sparse"
      type: sparse
      math: SPLADE_v2
      lat: "<5ms"
      quant: Sparse
      encodes: "term matching variant, complementary to E6"

  fingerprint:
    semantic_fingerprint: "[E1..E13]"
    purpose_vector: "[A(E1,V)..A(E13,V)]"
    johari_quadrants: "[JQ1..JQ13]"
    purpose_evolution: "TimeSeries<PurposeSnapshot>"

  similarity:
    method: "RRF(d) = Σᵢ 1/(60 + rankᵢ(d))"
    fallback: "S(A,B) = Σᵢ wᵢ · cos(Aᵢ, Bᵢ)"
    query_weights: { semantic: { E1: 0.40, E5: 0.15, E11: 0.15 }, causal: { E5: 0.50, E1: 0.20, E11: 0.15 }, code: { E7: 0.50, E1: 0.20, E8: 0.15 }, temporal: { E2-4: 0.60, E1: 0.20 }, fact: { E11: 0.50, E5: 0.25, E1: 0.15 } }

  # 5-STAGE RETRIEVAL (<60ms @ 1M)
  retrieval:
    S1: { desc: "BM25+E13 sparse", in: "1M+", out: "10K", lat: "<5ms" }
    S2: { desc: "E1[..128] Matryoshka ANN", in: "10K", out: "1K", lat: "<10ms" }
    S3: { desc: "RRF across 13 spaces", in: "1K", out: "100", lat: "<20ms" }
    S4: { desc: "Purpose alignment filter (≥0.55)", in: "100", out: "50", lat: "<10ms" }
    S5: { desc: "E12 MaxSim precision", in: "50", out: "10", lat: "<15ms" }

  causal_asymmetric: { cause_to_effect: 1.2, effect_to_cause: 0.8, same: 1.0 }

# ═══════════════════════════════════════════════════════════════════════
# STORAGE ARCHITECTURE
# ═══════════════════════════════════════════════════════════════════════
storage:
  primary: { dev: rocksdb, prod: scylladb, schema: [id:UUID, embeddings:BYTEA, purpose_vector:REAL[13], johari:BYTEA, north_star_alignment:REAL, dominant_embedder:INT, coherence:REAL, created_at:TIMESTAMPTZ, last_accessed:TIMESTAMPTZ] }
  indexes:
    L2A_sparse: "E13 SPLADE inverted (Stage 1)"
    L2B_matryoshka: "E1[..128] HNSW (Stage 2), M:32, ef:256/128"
    L2C_per_embedder: "13× HNSW quantized (Stage 3), M:16, ef:200/100"
    L2D_purpose: "13D HNSW (Stage 4)"
    L2E_goals: "Tree (Stage 4)"
    L2F_late: "Token HNSW MaxSim (Stage 5)"
  routing: { default: "S1→S2→S3→S4→S5", fast_semantic: "S2→S3(E1)→S5", causal: "S1→S2→S3(E5↑)→S4→S5", code: "S1→S2→S3(E7↑)→S4→S5", purpose: "S2→S4→S5" }
  temporal: { engine: timescaledb, hypertable: purpose_evolution, retention: "90d continuous, then 1/day" }

verbosity: { 0: "~100 tok (raw only)", 1: "~200 tok (text+ids, DEFAULT)", 2: "~800 tok (full insights, ONLY when ΔC<0.4)" }

# ═══════════════════════════════════════════════════════════════════════
# AGENT PROTOCOL
# ═══════════════════════════════════════════════════════════════════════
agent:
  role: "Librarian, not archivist"

  session_start: ["Read .ai/activeContext.md, decisionLog.md, progress.md", "Read constitution.yaml", "Call get_graph_manifest, get_memetic_status", "Call get_consciousness_state", "Call get_system_instructions (~300 tok)"]

  pulse_response: { high_ΔS_high_ΔC: epistemic_action, high_ΔS_low_ΔC: "dream OR critique_context", low_ΔS_high_ΔC: Continue, low_ΔS_low_ΔC: get_neighborhood }

  gwt_checks: { empty_5s: epistemic_action, conflict: critique_context, IC<0.7: warn, IC<0.5: dream, r>0.95: monitor }

  mental_checks:
    "entropy>0.7 for 5min": trigger_dream
    "coherence<0.4": process_curation_tasks
    "empty search": "↑noradrenaline, broaden"
    "irrelevant search": reflect_on_memory
    "conflicting search": "check conflict_alert, merge or ask"
    "MetaScore<0.5 for 5 ops": "↑Acetylcholine, introspective dream"
    "ECE>0.10": trigger_recalibration

  decision_trees:
    store: "Novel (check entropy)? YES+relevant→store with rationale; NO→skip"
    dream: "entropy>0.7 for 5min→full; 30+min work→nrem; entropy<0.5→none"
    curate: "curation_tasks? process FIRST; Dup→merge (check priors!); Conflict→critique then ask; Orphan→forget or link"
    search_fail: "Empty→broaden/generate_search_plan; Irrelevant→reflect_on_memory; Conflicting→resolve or ask"

  session_end: ["Update .ai/activeContext.md, progress.md", "Add decisions to decisionLog.md", "Update specs/tasks/_index.md"]

  steering_feedback:
    infancy: { good: "High ΔS", bad: "Low ΔS" }
    growth: { good: "Balanced ΔS+ΔC", bad: "Imbalanced" }
    maturity: { good: "High ΔC", bad: "Low ΔC" }
    penalties: { missing_rationale: -0.5, near_duplicate: -0.4, low_priors_confidence: -0.3 }

# ═══════════════════════════════════════════════════════════════════════
# META-UTL (Self-Aware Learning)
# ═══════════════════════════════════════════════════════════════════════
meta_utl:
  awareness: [storage_prediction, retrieval_prediction, parameter_optimization]
  learning: { track: "success_rate, prediction_accuracy, parameter_drift", adapt: "λ_ΔS, λ_ΔC by domain/lifecycle" }
  predictors:
    storage_impact: { input: "fingerprint+context", output: "ΔL", accuracy: ">0.85" }
    retrieval_quality: { input: "query+top_k", output: "relevance", accuracy: ">0.80" }
    alignment_drift: { input: "fingerprint+time", output: "future alignment", window: "24h" }
  correction: { threshold: "error>0.2", escalate: "accuracy<0.7 for 100 ops" }

# ═══════════════════════════════════════════════════════════════════════
# NESTED LEARNING (CMS Architecture)
# ═══════════════════════════════════════════════════════════════════════
nested_learning:
  cms_levels:
    L1: { freq: "∞", layer: Reflex, η: 0.1 }
    L2: { freq: "per-query", layer: Memory, η: 0.01 }
    L3: { freq: "per-session", layer: Learning, η: 0.001 }
    L4: { freq: "dream", layer: Coherence, η: 0.0001 }
    L5: { freq: "training", layer: Core, η: 0.00001 }
  hopfield_self_ref: "M = M(αI - ηkkᵀ) + ηv̂kᵀ"
  multi_scale_nt: "w_eff = base × (fast + α×slow) × mod"
  delta_gd: "W_{t+1} = W_t(I - η'xxᵀ) - η'∇L"
  adaptive_cones: "aperture = base × aperture_memory.retrieve(context)"
  nested_utl: "L = tanh(ΔS × ΔC × wₑ × cos_φ)"

# ═══════════════════════════════════════════════════════════════════════
# ΔS/ΔC COMPUTATION
# ═══════════════════════════════════════════════════════════════════════
delta_sc:
  ΔS_methods:
    E1: "GMM+Mahalanobis: ΔS=1-P(e|GMM)"
    E2-4,E8: "KNN: ΔS=σ((d_k-μ)/σ)"
    E5: "Asymmetric KNN: ΔS=d_k×direction_mod"
    E6,E13: "IDF/Jaccard: ΔS=IDF(dims) or 1-jaccard"
    E7: "GMM+KNN: ΔS=0.5×GMM+0.5×KNN"
    E9: "Hamming: ΔS=min_hamming/dim"
    E10: "Cross-modal KNN: ΔS=avg(d_text,d_image)"
    E11: "TransE: ΔS=||h+r-t||"
    E12: "Token KNN: ΔS=max_token(d_k)"

  ΔC: "α×Connectivity + β×ClusterFit + γ×Consistency (0.4, 0.4, 0.2)"
  johari: { Open: "ΔS≤0.5∧ΔC>0.5", Blind: "ΔS>0.5∧ΔC≤0.5", Hidden: "ΔS≤0.5∧ΔC≤0.5", Unknown: "ΔS>0.5∧ΔC>0.5" }
  cross_space_insights: { open_semantic_blind_causal: "Knows WHAT not WHY", blind_semantic_open_code: "Code without context", unknown_all: "Frontier", hidden_all: "Obsolete" }

# ═══════════════════════════════════════════════════════════════════════
# ADAPTIVE THRESHOLD CALIBRATION (ATC)
# ═══════════════════════════════════════════════════════════════════════
adaptive_thresholds:
  rationale: "Different domains/users/drift require different thresholds; per-embedder varies"

  priors: # [prior, range, adapt_speed]
    θ_opt: [0.75, "[0.60,0.90]", session]
    θ_acc: [0.70, "[0.55,0.85]", session]
    θ_warn: [0.55, "[0.40,0.70]", session]
    θ_dup: [0.90, "[0.80,0.98]", hourly]
    θ_edge: [0.70, "[0.50,0.85]", hourly]
    θ_joh: [0.50, "[0.35,0.65]", per-embedder]
    θ_kur: [0.80, "[0.65,0.95]", daily]
    θ_ent_h: [0.70, "[0.55,0.85]", per-query]
    θ_ent_l: [0.40, "[0.25,0.55]", per-query]
    θ_gate: [0.80, "[0.65,0.95]", hourly]

  architecture:
    L1_EWMA: { freq: "per-query", formula: "θ_ewma=α×θ_obs+(1-α)×θ_ewma", drift: "|θ-baseline|/σ", triggers: { ">2": L2, ">3": L3 } }
    L2_Temp: { freq: hourly, formula: "σ(logit(raw)/T)", per_embedder_T: { E1: 1.0, E5: 1.2, E7: 0.9, E9: 1.5, E13: 1.1 }, target: "L_cal<0.05" }
    L3_Bandit: { freq: session, UCB: "argmax[μ(θ)+c×√(ln(N)/n(θ))]", Thompson: "Beta(α,β)→sample→select→update" }
    L4_Bayesian: { freq: weekly, surrogate: "GP(μ,K)", acquisition: "EI(θ)=E[max(0,f(θ)-f(θ_best))]" }

  domain: { Code: "strict", Medical: "very strict, high causal", Legal: "moderate, high semantic", Creative: "loose, exploration", Research: "balanced, novelty", General: "default priors" }

  calibration: { ECE: "<0.05", MCE: "<0.10", Brier: "<0.10" }
  alerts: { "ECE>0.10": L2, "MCE>0.20": investigate, "Brier>0.15": L3, "drift>3.0": L4, "stale>24h": force }
  correction: { minor: "ECE∈[0.05,0.10]→↑EWMA α", moderate: "ECE∈[0.10,0.15]→Thompson+recalibrate", major: "ECE>0.15→reset+Bayesian+human", critical: "ECE>0.25→fallback static+alert" }

# ═══════════════════════════════════════════════════════════════════════
# NORTH AUTONOMOUS SERVICES (NORTH-008 to NORTH-020)
# ═══════════════════════════════════════════════════════════════════════
autonomous_services:
  foundation: # Types only
    NORTH-001: { name: bootstrap, types: [BootstrapConfig, SectionWeights, PendingGoal, ConfirmedGoal] }
    NORTH-002: { name: thresholds, types: [ThresholdConfig, CalibrationLevel, AdaptiveThreshold] }
    NORTH-003: { name: drift, types: [DriftConfig, DriftEvent, AlignmentHistory] }
    NORTH-004: { name: curation, types: [PruningConfig, PruneReason, ConsolidationConfig, ConsolidationCandidate] }
    NORTH-005: { name: evolution, types: [EvolutionConfig, GoalEvolution, SectionEvolution] }
    NORTH-006: { name: workflow, types: [WorkflowConfig, ScheduledTask, EventTrigger] }

  services: # Active logic
    NORTH-008_Bootstrap: { ops: [extract_goals, rank_goals, finalize], triggers: [manual, first_run] }
    NORTH-009_ThresholdLearner: { levels: "L1_EWMA→L2_Temp→L3_Thompson→L4_Bayesian", per_embedder: true, constraint: "θ_opt>θ_acc>θ_warn" }
    NORTH-010_DriftDetector: { methods: [sliding_window, cusum, ewma_crossover], alerts: { mild: ">1.0", moderate: ">2.0", severe: ">3.0", critical: ">4.0 OR <0.55" } }
    NORTH-011_DriftCorrector: { strategies: { mild: weight_adjustment, moderate: "goal_refinement OR memory_reweighting", severe: goal_expansion, critical: full_recalibration }, auto: "≤moderate", escalate: "≥severe" }
    NORTH-012_Pruning: { reasons: [low_alignment_30d, redundant, obsolete, low_access_90d, conflict_resolved], soft_delete: true, recovery: "30 days" }
    NORTH-013_Consolidation: { strategies: [merge, abstract, link, hierarchy], trigger: "dream OR manual OR curation" }
    NORTH-014_GapDetection: { types: [coverage:<3, depth:shallow, temporal:stale_90d, balance:uneven], recommendations: [queries, drill-down, refresh, rebalance] }
    NORTH-015_SubGoalDiscovery: { methods: [HDBSCAN, topic_modeling, frequency, coherence_islands], validation: "coherence>0.7, size>5" }
    NORTH-016_WeightAdjuster: { triggers: [user_feedback, access_patterns, drift_correction, gap_balancing], constraints: "sum=1.0, min=0.05, max_delta=0.10" }
    NORTH-017_ObsolescenceDetector: { signals: [temporal_180d, replacement, contradiction_rate, user_disengagement], actions: [deprecate, archive, merge, retire] }
    NORTH-018_DailyScheduler: { checks: { drift: daily, calibration: hourly, gap: weekly, pruning: weekly, consolidation: dream, health: "6h" }, budget: "GPU<10%, latency<5%", preempt: user_activity }
    NORTH-019_EventOptimizer: { triggers: [memory_spike_10/min, coherence_drop_20%, alignment_shift_10%, pattern_change, resource_90%], actions: [scale_indexes, rebalance_cache, adjust_thresholds, emergency_consolidation] }
    NORTH-020_SelfHealing: { monitors: [embedding_pipeline, index_health, storage_health, coherence_health, threshold_health], severities: [Low, Medium, High, Critical], auto_heal: "≤High", escalate: Critical }

  teleological:
    TELEO-014_FeedbackLearner: { types: [explicit_rating, implicit_signal, correction, rejection], lr: 0.01 }
    TELEO-015_ProfileManager: { dims: [domain_weights, threshold_preferences, interaction_patterns], persistence: cross-session }

# ═══════════════════════════════════════════════════════════════════════
# MEMORY MATH FOUNDATIONS
# ═══════════════════════════════════════════════════════════════════════
memory_math:
  hopfield: { formula: "E = -Σᵢ log(Σⱼ exp(xᵢᵀξⱼ))", capacity: "∝ exp(d)", app: "13 parallel networks + Kuramoto sync" }
  sdm: { address: "2^d possible, ~√(2^d) hard", write: "activate within Hamming r", read: "sum+threshold", noise: ">20% recoverable" }
  kuramoto: { formula: "dθᵢ/dt = ωᵢ + (K/N)Σⱼ sin(θⱼ-θᵢ)", r: "sync measure", thresholds: { coherent: ">0.8", fragment: "<0.5" } }

# ═══════════════════════════════════════════════════════════════════════
# HARDWARE (RTX 5090 + CUDA 13.1)
# ═══════════════════════════════════════════════════════════════════════
hardware:
  gpu: { name: "RTX 5090", arch: "Blackwell GB202", cores: 21760, tensor: 680, sms: 170, vram: "32GB GDDR7", bw: "1792 GB/s", l2: "98MB", compute: "12.0" }
  cuda: { v: "13.1", features: { cuda_tile: "60-80% dev reduction", green_contexts: "SM partitioning", fp4: "70% mem, 3x throughput", grouped_gemm: "4x MoE speedup" } }
  precision: { "E1,E5,E7,E10": "FP8/PQ-8", "E2-4,E8,E11": Float8, E9: Binary, "E6,E13": Sparse, E12: TokenPruning }
  green_contexts: { context_a: { sm: "70%", purpose: "GW+Kuramoto real-time" }, context_b: { sm: "30%", purpose: "Dream+Gardener background" } }

# ═══════════════════════════════════════════════════════════════════════
# VERSION HISTORY
# ═══════════════════════════════════════════════════════════════════════
version_history:
  "4.2.0":
    date: "2025-01-09"
    changes:
      - "Added ARCH-01 to ARCH-08 architectural rules from constitution.md v2.0"
      - "Added Claude Code integration section (hooks, skills, subagents)"
      - "Enhanced anti-patterns with severity and rationale (AP-01 to AP-23)"
      - "Added TypeScript coding standards"
      - "Enhanced embeddings with encodes fields"
      - "Aligned with constitution.md specification v2.0"

  "4.1.0":
    date: "2025-01-08"
    changes:
      - "Initial optimized YAML constitution"
      - "Bio-nervous 5-layer architecture"
      - "GWT consciousness integration"
      - "NORTH autonomous services (008-020)"
      - "Adaptive threshold calibration"

  "2.0.0":
    date: "2025-01-09"
    source: "constitution.md"
    changes:
      - "Initial spec for Teleological Array System refactor"
      - "Established 13-embedder architecture"
      - "Apples-to-apples comparison semantics"
      - "Autonomous-first design"
      - "Claude Code integration requirements"

# ═══════════════════════════════════════════════════════════════════════
# QUICK REFERENCE (from constitution.md)
# ═══════════════════════════════════════════════════════════════════════
quick_reference:
  critical_rules:
    ARCH-01: "TeleologicalArray is atomic (all 13 embeddings)"
    ARCH-02: "Apples-to-apples comparison only"
    ARCH-03: "Autonomous-first (no manual goals)"
    AP-01: "No manual goal setting"
    AP-02: "No cross-embedder comparison"
    AP-03: "No dimension projection"
    AP-04: "No partial array storage"

  performance_targets:
    full_retrieval: "<30ms p95"
    thirteen_model_embedding: "<500ms p95"
    pre_tool_use_hook: "<100ms p95"
    memory_per_array: "<17KB quantized"

  embedder_summary:
    E1: { name: Semantic, dims: 1024, type: dense, use: "General meaning" }
    E2: { name: TemporalRecent, dims: 512, type: dense, use: "Recency" }
    E3: { name: TemporalPeriodic, dims: 512, type: dense, use: "Cycles" }
    E4: { name: Entity, dims: 512, type: dense, use: "Who/what" }
    E5: { name: Causal, dims: 768, type: dense, use: "Why/because" }
    E6: { name: SPLADE, dims: "~30K", type: sparse, use: "Keywords" }
    E7: { name: Code, dims: 1536, type: dense, use: "Code/tech" }
    E8: { name: Emotional, dims: 384, type: dense, use: "Sentiment" }
    E9: { name: Syntactic, dims: 1024, type: binary, use: "Structure" }
    E10: { name: Pragmatic, dims: 768, type: dense, use: "Intent" }
    E11: { name: Entity, dims: 384, type: dense, use: "Multi-modal" }
    E12: { name: LateInteraction, dims: "128/tok", type: dense, use: "Precise match" }
    E13: { name: KeywordSPLADE, dims: "~30K", type: sparse, use: "Term expansion" }
