# Context Graph Constitution v6.4.0
# 13-Perspectives Collaboration | E1 Foundation | GPU-First Architecture
# ═══════════════════════════════════════════════════════════════

# GPU-FIRST ARCHITECTURE
# This is a GPU-first system. All compute-intensive operations use GPU over CPU.
# All 13 embedders are warm-loaded into GPU memory at MCP server startup.
# Target Hardware: RTX 5090 32GB (Blackwell GB202) + CUDA 13.1
#
# Key GPU Capabilities:
#   - 32GB GDDR7 VRAM: All 13 embedder models resident simultaneously
#   - 1,792 GB/s bandwidth: Sub-millisecond embedding generation
#   - 680 5th-gen Tensor Cores: FP16/BF16/FP8 inference
#   - 170 SMs with Green Contexts: Isolated inference partitions
#   - CUDA Tile: Portable, auto-optimized kernels

# CORE PHILOSOPHY: 13 embedders = 13 unique perspectives on every memory
# Each finds what OTHERS MISS. Combined = superior answers.
#
# Example: Query "What databases work with Rust?"
# - E1 finds: "database" or "Rust" semantically
# - E11 finds: "Diesel" (knows Diesel IS a database ORM - E1 missed this)
# - E7 finds: code using sqlx, diesel crates
# - Combined: Better answer than any single embedder

# ═══════════════════════════════════════════════════════════════
# EMBEDDERS (13 Perspectives) - ALL GPU-RESIDENT
# ═══════════════════════════════════════════════════════════════
# All 13 embedders are warm-loaded into GPU VRAM at startup.
# Total VRAM footprint: ~8-12GB (leaves 20GB+ for batch processing)
# Inference precision: FP16/BF16 (Tensor Core accelerated)
# No CPU fallback - GPU is mandatory for this system.

embedders:
  # FOUNDATION (GPU: ~2GB VRAM)
  E1: { name: V_meaning, dim: 1024, finds: "Semantic similarity", misses: "Entities, code patterns, causal links", gpu: "warm-loaded" }

  # SEMANTIC ENHANCERS (topic_weight: 1.0, GPU: ~4GB total)
  E5: { name: V_causality, dim: 768, finds: "Causal chains (why X caused Y)", misses_by_E1: "Direction lost", gpu: "warm-loaded" }
  E6: { name: V_selectivity, sparse: true, finds: "Exact keyword matches", misses_by_E1: "Diluted by averaging", gpu: "warm-loaded" }
  E7: { name: V_correctness, dim: 1536, finds: "Code patterns, function signatures", misses_by_E1: "Treats code as NL", gpu: "warm-loaded" }
  E10: { name: V_multimodality, dim: 768, finds: "Same-goal work (different words)", integration: "Multiplicative boost on E1", gpu: "warm-loaded" }
  E12: { name: V_precision, per_token: true, finds: "Exact phrase matches", use: "Reranking only", gpu: "warm-loaded" }
  E13: { name: V_keyword_precision, sparse: true, finds: "Term expansions (fast→quick)", use: "Stage 1 recall", gpu: "warm-loaded" }

  # RELATIONAL ENHANCERS (topic_weight: 0.5, GPU: ~2GB total)
  E8: { name: V_connectivity, dim: 384, finds: "Graph structure (X imports Y)", gpu: "warm-loaded" }
  E11: { name: V_factuality, dim: 768, finds: "Entity knowledge (Diesel=database ORM)", model: "KEPLER (RoBERTa-base + TransE)", gpu: "warm-loaded" }

  # TEMPORAL CONTEXT (topic_weight: 0.0, POST-RETRIEVAL ONLY, GPU: ~1.5GB total)
  E2: { name: V_freshness, dim: 512, finds: "Recency", gpu: "warm-loaded" }
  E3: { name: V_periodicity, dim: 512, finds: "Time-of-day patterns", gpu: "warm-loaded" }
  E4: { name: V_ordering, dim: 512, finds: "Sequence (before/after)", gpu: "warm-loaded" }

  # STRUCTURAL (topic_weight: 0.5, GPU: ~1GB)
  E9: { name: V_robustness, dim: 1024, finds: "Noise-robust structure", gpu: "warm-loaded" }

# ═══════════════════════════════════════════════════════════════
# ARCHITECTURAL RULES (MUST follow)
# ═══════════════════════════════════════════════════════════════
arch_rules:
  # GPU-First (MANDATORY)
  ARCH-GPU-01: "GPU is mandatory - no CPU fallback for embeddings"
  ARCH-GPU-02: "All 13 embedders warm-loaded into VRAM at MCP server startup"
  ARCH-GPU-03: "Embedding inference uses FP16/BF16 Tensor Cores"
  ARCH-GPU-04: "FAISS indexes use GPU (faiss-gpu) not CPU"
  ARCH-GPU-05: "HDBSCAN clustering runs on GPU via cuML"
  ARCH-GPU-06: "Batch operations preferred - minimize kernel launches"
  ARCH-GPU-07: "Green Contexts partition SMs: 70% inference, 30% indexing"
  ARCH-GPU-08: "CUDA streams for async embedding + indexing overlap"

  # Core
  ARCH-01: "TeleologicalArray is atomic - all 13 embeddings or nothing"
  ARCH-02: "Apples-to-apples only - compare E1↔E1, never E1↔E5"
  ARCH-04: "Temporal (E2-E4) NEVER count toward topics"
  ARCH-05: "All 13 embedders required - missing = fatal"
  ARCH-06: "All memory ops through MCP tools only"
  ARCH-09: "Topic threshold: weighted_agreement >= 2.5"
  ARCH-10: "Divergence detection: SEMANTIC embedders only (E1,E5,E6,E7,E10,E12,E13)"

  # Retrieval Pipeline
  ARCH-12: "E1 is foundation - all retrieval starts with E1"
  ARCH-13: "Strategies: E1Only (default), MultiSpace (E1+enhancers), Pipeline (E13→E1→E12)"
  ARCH-17: "Strong E1 (>0.8): enhancers refine. Weak E1 (<0.4): enhancers broaden"
  ARCH-18: "E5 Causal: asymmetric similarity (cause→effect direction matters)"
  ARCH-21: "Multi-space fusion: use Weighted RRF, not weighted sum"

  # E10 Multiplicative Boost
  ARCH-28: "E10 uses multiplicative boost: E1 * (1 + boost), NOT linear blending"
  ARCH-29: "E10 boost adapts: strong E1=5%, medium=10%, weak=15%"
  ARCH-30: "E10 alignment: >0.5=boost, <0.5=reduce, =0.5=neutral"
  ARCH-33: "E10 multiplier clamped to [0.8, 1.2]"

  # Temporal (POST-RETRIEVAL ONLY)
  ARCH-25: "Temporal boosts POST-retrieval only, NOT in similarity fusion"

# ═══════════════════════════════════════════════════════════════
# ANTI-PATTERNS (FORBIDDEN)
# ═══════════════════════════════════════════════════════════════
forbidden:
  # GPU Anti-Patterns
  AP-GPU-01: "NEVER use CPU for embedding inference when GPU available"
  AP-GPU-02: "NEVER cold-load embedders per-request - warm-load at startup"
  AP-GPU-03: "NEVER use CPU FAISS when GPU FAISS available"
  AP-GPU-04: "NEVER use sklearn HDBSCAN - use cuML GPU implementation"
  AP-GPU-05: "NEVER transfer embeddings GPU→CPU→GPU - keep on GPU"
  AP-GPU-06: "NEVER use FP32 for inference - use FP16/BF16 Tensor Cores"
  AP-GPU-07: "NEVER serialize embeddings per-item - batch for GPU efficiency"
  AP-GPU-08: "NEVER block on sync - use CUDA streams for async ops"

  # Core Anti-Patterns
  AP-02: "No cross-embedder comparison (E1↔E5)"
  AP-04: "No partial TeleologicalArray"
  AP-05: "No embedding fusion into single vector"
  AP-60: "Temporal (E2-E4) MUST NOT count toward topics"
  AP-73: "Temporal MUST NOT be used in similarity fusion"
  AP-74: "E12 ColBERT: reranking ONLY, not initial retrieval"
  AP-75: "E13 SPLADE: Stage 1 recall ONLY, not final ranking"
  AP-77: "E5 MUST NOT use symmetric cosine - causal is directional"
  AP-79: "MUST NOT use simple weighted sum - use Weighted RRF"
  AP-80: "E10 MUST NOT use linear blending - makes E10 compete with E1"
  AP-84: "E10 MUST NOT override E1 - when E1=0, result=0"

# ═══════════════════════════════════════════════════════════════
# TOPIC SYSTEM
# ═══════════════════════════════════════════════════════════════
topics:
  weighted_agreement:
    formula: "Sum(topic_weight_i × is_clustered_i)"
    threshold: 2.5
    max: 8.5  # 7×1.0 (semantic) + 2×0.5 (relational) + 1×0.5 (structural)
    temporal_contribution: 0.0  # ALWAYS

  categories:
    SEMANTIC: { embedders: [E1,E5,E6,E7,E10,E12,E13], weight: 1.0 }
    RELATIONAL: { embedders: [E8,E11], weight: 0.5 }
    STRUCTURAL: { embedders: [E9], weight: 0.5 }
    TEMPORAL: { embedders: [E2,E3,E4], weight: 0.0 }  # Never for topics

  divergence_detection: "SEMANTIC embedders only"

# ═══════════════════════════════════════════════════════════════
# MCP TOOLS
# ═══════════════════════════════════════════════════════════════
mcp_tools:
  # CORE SEARCH: Primary retrieval operations
  search: [search_graph, search_causes, search_connections, search_by_intent]

  # EMBEDDER-FIRST SEARCH: Any embedder as primary perspective (NEW)
  # Enables AI agents to explore the knowledge graph from any of the 13 perspectives
  embedder_search:
    - search_by_embedder      # Generic: search using any embedder (E1-E13) as primary
    - get_embedder_clusters   # Explore clusters in any embedder's space
    - compare_embedder_views  # Compare how different embedders see the same query
    - list_embedder_indexes   # List available embedders and their index stats

  # SPECIALIZED SEARCH: Per-embedder tools with custom logic
  semantic: [search_graph]              # E1
  causal: [search_causes, get_causal_chain]    # E5 with asymmetric similarity
  keyword: [search_by_keywords]         # E6 sparse
  code: [search_code]                   # E7 code-aware
  graph: [search_connections, get_graph_path]  # E8 structural
  entity: [search_by_entities, extract_entities, infer_relationship, find_related_entities, validate_knowledge, get_entity_graph]  # E11 KEPLER
  intent: [search_by_intent, find_contextual_matches]  # E10 multiplicative boost

  # MEMORY OPERATIONS
  memory: [store_memory, inject_context, get_memetic_status]
  sequence: [get_conversation_context, get_session_timeline, traverse_memory_chain]

  # TOPIC & CLUSTER OPERATIONS
  topic: [get_topic_portfolio, get_topic_stability, detect_topics, get_divergence_alerts]

  # MAINTENANCE
  maintenance: [trigger_consolidation, trigger_dream, merge_concepts, forget_concept, boost_importance]

  search_strategies:
    E1Only: "Fast, simple semantic queries"
    MultiSpace: "E1 + enhancers via RRF - use when E1 blind spots matter"
    Pipeline: "E13 recall → E1 dense → E12 rerank - maximum precision"
    EmbedderFirst: "Any embedder as primary perspective - explore blind spots"

# ═══════════════════════════════════════════════════════════════
# EMBEDDER-FIRST SEARCH (NEW)
# ═══════════════════════════════════════════════════════════════
# Core insight: Each embedder sees the knowledge graph from a unique perspective.
# Sometimes E1 (semantic) misses what E11 (entity) finds, or E7 (code) discovers
# patterns E5 (causal) doesn't see. Embedder-first search lets AI agents choose
# which perspective to prioritize for a given query.

embedder_first_search:
  philosophy: |
    The 13 embedders are 13 lenses on the same knowledge. By default, E1 (semantic)
    is the foundation, but sometimes another perspective reveals what E1 misses.

    Example: "What framework does Tokio relate to?"
    - E1 (semantic): finds "async", "runtime" → generic matches
    - E11 (entity): finds "Rust", "Actix" → entity relationships
    - E8 (graph): finds "imports", "depends on" → structural relationships
    - E7 (code): finds "tokio::spawn", "#[tokio::main]" → code patterns

    Embedder-first search lets the AI choose the most relevant perspective.

  tool_specs:
    search_by_embedder:
      description: "Search using any embedder (E1-E13) as the primary perspective"
      params:
        query: "Search query"
        embedder: "E1|E2|E3|E4|E5|E6|E7|E8|E9|E10|E11|E12|E13"
        topK: "1-100 (default 10)"
        minSimilarity: "0.0-1.0 (default 0.0)"
        includeContent: "boolean (default false)"
        includeAllScores: "boolean - return scores from all 13 embedders"
      use_cases:
        - "E11 search when looking for entity relationships E1 misses"
        - "E7 search when looking for code patterns"
        - "E5 search for causal relationships without asymmetric complexity"
        - "E8 search for graph structure (imports, dependencies)"

    get_embedder_clusters:
      description: "Explore clusters in a specific embedder's space"
      params:
        embedder: "E1|E2|...E13"
        minClusterSize: "default 3"
        topClusters: "default 10"
      use_cases:
        - "Find code clusters (E7) to see related implementations"
        - "Find entity clusters (E11) to see related concepts"
        - "Find temporal clusters (E2) to see recency patterns"

    compare_embedder_views:
      description: "Compare how different embedders rank the same query"
      params:
        query: "Search query"
        embedders: "[E1, E5, E7, E11]" # Subset to compare
        topK: "default 5"
      output: |
        Shows rankings from each embedder side-by-side:
        - E1 top 5: [memory1, memory2, ...]
        - E5 top 5: [memory3, memory1, ...]
        - Agreement score: how much embedders agree
        - Unique finds: memories found by only one embedder
      use_cases:
        - "Understand E1 blind spots by comparing with E11"
        - "See what causal (E5) finds that semantic (E1) misses"

    list_embedder_indexes:
      description: "List all embedder indexes with stats"
      output:
        per_embedder:
          - embedder: "E1-E13"
          - dimension: "vector dimension or 'sparse'"
          - index_type: "HNSW|Inverted|MaxSim"
          - vector_count: "number of indexed vectors"
          - index_size_mb: "approximate size"
          - gpu_resident: "true|false"

  # Per-embedder direct search (what each finds)
  embedder_perspectives:
    E1_semantic: "Dense semantic similarity - foundation"
    E2_recency: "Temporal freshness - recent memories first"
    E3_periodic: "Time-of-day patterns - daily/weekly cycles"
    E4_sequence: "Conversation order - before/after relationships"
    E5_causal: "Cause-effect relationships - why X caused Y"
    E6_keyword: "Exact keyword matches - precise terminology"
    E7_code: "Code patterns - function signatures, AST structure"
    E8_graph: "Structural relationships - imports, dependencies"
    E9_hdc: "Noise-robust structure - typos, variations"
    E10_intent: "Goal alignment - similar purpose, different words"
    E11_entity: "Entity knowledge - named entities, relationships"
    E12_precision: "Exact phrase matches - token-level precision"
    E13_expansion: "Term expansion - synonyms, related terms"

# ═══════════════════════════════════════════════════════════════
# RETRIEVAL PIPELINE
# ═══════════════════════════════════════════════════════════════
retrieval:
  stages:
    S1: "E13 SPLADE sparse recall → 10K candidates"
    S2: "E1 Matryoshka ANN → 1K candidates"
    S3: "RRF across semantic spaces → 100 candidates"
    S4: "Topic alignment (weighted_agreement >= 2.5) → 50"
    S5: "E12 MaxSim rerank → 10 final results"

  # When to use which enhancer
  use_E5: "Causal queries (why, what caused)"
  use_E7: "Code queries (implementations, functions)"
  use_E10: "Intent queries (same goal, similar purpose)"
  use_E11: "Entity queries (specific named things)"
  use_E6_E13: "Keyword queries (exact terms, jargon)"

# ═══════════════════════════════════════════════════════════════
# AUTONOMOUS MULTI-EMBEDDER ENRICHMENT
# ═══════════════════════════════════════════════════════════════
# Simplify tool calls: system auto-detects query type and selects embedders.
# Fewer calls needed, richer results per call.

autonomous_enrichment:
  philosophy: |
    Each of the 13 embedders looks at a query from its unique angle.
    E1 (semantic) is foundation but has blind spots. Other embedders find what E1 misses:
    - E5: "the bug that caused the crash" (causal relationships)
    - E7: structural code patterns (not just word similarity)
    - E10: same goal, different words (intent alignment)
    - E11: "Diesel" = database ORM (entity knowledge E1 lacks)

    Combined: Better answers than any single embedder alone.

  modes:
    Off: "E1-only search (legacy behavior)"
    Light: "E1 + 1-2 enhancers, basic agreement metrics (default)"
    Full: "All relevant embedders, full metrics, blind spot detection"

  query_type_detection:
    # System auto-detects query type from patterns
    CAUSAL: ["why", "caused", "because", "led to", "root cause"]
    CODE: ["::", "->", "fn ", "function", "impl", ".await"]
    ENTITY: ["what is", "works with", "related to", capitalized_names]
    INTENT: ["goal", "purpose", "trying to", "accomplish"]
    KEYWORD: [quoted_terms, "exact", "error:", "v2."]
    TEMPORAL: ["yesterday", "before", "after", "recently"]

  embedder_selection:
    CAUSAL: [E5, E8]       # E5 causal + E8 for causal chains
    CODE: [E7, E6]         # E7 code + E6 keywords for function names
    ENTITY: [E11, E6]      # E11 entity + E6 keywords
    INTENT: [E10, E5]      # E10 intent + E5 often overlap
    KEYWORD: [E6, E13]     # E6 sparse + E13 SPLADE expansion
    TEMPORAL: []           # E2-E4 are POST-RETRIEVAL only (ARCH-25)

  output_enrichment:
    scoring_breakdown:
      - e1_score: "E1 (semantic) similarity"
      - enhancer_scores: "Per-enhancer scores {e5: 0.8, e7: 0.6}"
      - rrf_final: "Weighted RRF fusion score (per ARCH-21)"
      - e10_boost: "E10 multiplicative boost if applied"
    agreement_metrics:
      - embedders_agree: "[E1, E5, E11]"
      - agreement_score: "0.6 (3 of 5 embedders)"
      - weighted_agreement: "2.5 (topic threshold)"
    blind_spot_alert:
      - node_id: "Memory found by enhancer but missed by E1"
      - found_by: "[E11]"
      - e1_score: "0.05 (E1 missed this)"
      - enhancer_score: "0.85 (E11 found it)"
      - explanation: "E11 knows Diesel IS a database ORM"

  arch_rules:
    ARCH-ENRICH-01: "Query type detection happens before search"
    ARCH-ENRICH-02: "E1 always runs first (per ARCH-12)"
    ARCH-ENRICH-03: "Enhancers run in parallel via tokio::join!"
    ARCH-ENRICH-04: "Fusion uses Weighted RRF (per ARCH-21)"
    ARCH-ENRICH-05: "Blind spots: enhancer > 0.5 AND E1 < 0.3"
    ARCH-ENRICH-06: "Light mode: max 2 enhancers, Full mode: max 6"

  performance_budget:
    Light: "<500ms total"
    Full: "<800ms total"
    breakdown:
      e1_search: "~100ms"
      parallel_enhancers: "~150ms (run in parallel)"
      rrf_fusion: "~50ms"
      agreement_calc: "~50ms"
      buffer: "~150ms"

# ═══════════════════════════════════════════════════════════════
# MEMORY SOURCES
# ═══════════════════════════════════════════════════════════════
memory_sources:
  HookDescription: "Claude's description of tool use"
  ClaudeResponse: "Session summaries, significant responses"
  MDFileChunk: "Markdown file chunks (200 words, 50 overlap)"

# ═══════════════════════════════════════════════════════════════
# DREAM CONSOLIDATION
# ═══════════════════════════════════════════════════════════════
dream:
  trigger: "entropy > 0.7 AND churn > 0.5"
  nrem: "Hebbian replay - strengthen high-importance edges"
  rem: "Hyperbolic random walk - discover blind spots"

# ═══════════════════════════════════════════════════════════════
# KEY THRESHOLDS
# ═══════════════════════════════════════════════════════════════
thresholds:
  topic: 2.5           # weighted_agreement for topic detection
  high_sim: 0.75       # High similarity
  low_sim: 0.30        # Divergence threshold
  duplicate: 0.90      # Duplicate detection
  entropy_dream: 0.70  # Dream trigger
  churn_dream: 0.50    # Dream trigger

# ═══════════════════════════════════════════════════════════════
# HOOKS (Native Claude Code)
# ═══════════════════════════════════════════════════════════════
hooks:
  config: ".claude/settings.json"
  SessionStart: "Load topic portfolio, warm GPU indexes"
  UserPromptSubmit: "GPU embed prompt, search, detect divergence, inject context"
  PreToolUse: "Inject brief relevant context"
  PostToolUse: "Capture + GPU embed as HookDescription"
  Stop: "Capture response summary"
  SessionEnd: "Persist state, GPU HDBSCAN, check dream triggers"

# ═══════════════════════════════════════════════════════════════
# GPU INFRASTRUCTURE (RTX 5090 32GB + CUDA 13.1)
# ═══════════════════════════════════════════════════════════════
gpu:
  hardware:
    device: "NVIDIA GeForce RTX 5090"
    architecture: "Blackwell (GB202)"
    vram: "32GB GDDR7"
    bandwidth: "1,792 GB/s"
    cuda_cores: 21760
    tensor_cores: 680
    sms: 170
    compute_capability: "12.0"
    driver: "R580+ (CUDA 13.1)"

  host:
    cpu: "AMD Ryzen 9 9950X3D"
    cores: "16 cores / 32 threads"
    ram: "128GB DDR5"
    l3_cache: "192MB (96MB x2)"

  vram_allocation:
    embedders: "~10GB (all 13 warm-loaded)"
    faiss_indexes: "~8GB (HNSW per-space)"
    batch_buffers: "~4GB (inference batches)"
    cuml_workspace: "~2GB (HDBSCAN, clustering)"
    reserved: "~8GB (headroom for spikes)"
    total_budget: "32GB"

  performance_targets:
    all_13_embed: "<200ms (GPU batch)"
    per_space_hnsw: "<1ms (faiss-gpu)"
    inject_context_p95: "<500ms"
    store_memory_p95: "<800ms"
    any_tool_p99: "<1000ms"
    topic_detection: "<20ms (cuML HDBSCAN)"
    warm_load_startup: "<30s (all 13 models)"

  cuda_features:
    green_contexts: true  # SM partitioning for QoS
    cuda_tile: true       # Portable kernel programming
    cuda_streams: true    # Async overlap
    tensor_cores: "FP16/BF16/FP8"
    mps_clients: 60       # Multi-Process Service

  testing:
    all_tests_use_gpu: true
    benchmarks_require_gpu: true
    no_cpu_fallback_tests: true
    gpu_memory_profiling: true
