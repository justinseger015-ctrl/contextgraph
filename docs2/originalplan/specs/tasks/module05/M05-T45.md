# M05-T45: Implement store_memory UTL Validation and Steering Feedback

## Task Metadata
- **ID**: M05-T45
- **Title**: Implement store_memory UTL Validation and Steering Feedback
- **Module**: 05 - UTL Integration
- **Layer**: integration
- **Priority**: high
- **Estimated Hours**: 3

## Description

Integrate UTL into store_memory MCP tool with steering feedback.
Per PRD Section 8, store_memory response must include steering_reward.

Algorithm:
1. Compute UTL for incoming memory content
2. Validate priors_vibe_check compatibility if provided
3. Apply lifecycle-aware storage decision (should_store)
4. Compute steering_reward based on delta_s/delta_c and lifecycle stage
5. Return steering feedback in response

Steering rewards per lifecycle:
- Infancy: +reward for high delta_s (novelty)
- Growth: Balanced delta_s/delta_c
- Maturity: +reward for high delta_c (coherence)

Universal penalties:
- near-duplicate: -0.4
- low priors confidence: -0.3
- missing rationale: -0.5

## File Path
`crates/context-graph-mcp/src/tools/store_memory.rs`

## Dependencies
- M05-T22: UtlProcessor orchestrator
- M05-T19: LifecycleManager state machine
- M05-T36: Steering subsystem hooks

## Acceptance Criteria
- [ ] store_memory computes UTL before storage
- [ ] steering_reward field in response
- [ ] Lifecycle-aware reward computation
- [ ] Universal penalty rules enforced
- [ ] priors_vibe_check validation integrated
- [ ] Rationale required (missing = -0.5 penalty)
- [ ] Backward compatible response schema

## Test File
`crates/context-graph-mcp/tests/store_memory_utl_tests.rs`

## Specification References
- PRD Section 8 (Steering Subsystem)
- constitution.yaml steering.reward

## Implementation Notes

### Steering Reward Computation

```rust
pub struct SteeringReward {
    pub value: f32,           // Total reward in [-1.0, 1.0]
    pub components: Vec<RewardComponent>,
    pub lifecycle_stage: LifecycleStage,
}

pub struct RewardComponent {
    pub source: String,
    pub contribution: f32,
    pub reason: String,
}

impl SteeringRewardCalculator {
    pub fn compute_reward(
        &self,
        learning_signal: &LearningSignal,
        lifecycle: &LifecycleStage,
        has_rationale: bool,
        priors_confidence: Option<f32>,
        is_near_duplicate: bool,
    ) -> SteeringReward {
        let mut total = 0.0;
        let mut components = Vec::new();

        // Universal penalties
        if !has_rationale {
            total -= 0.5;
            components.push(RewardComponent {
                source: "rationale".into(),
                contribution: -0.5,
                reason: "Missing rationale for memory storage".into(),
            });
        }

        if is_near_duplicate {
            total -= 0.4;
            components.push(RewardComponent {
                source: "novelty".into(),
                contribution: -0.4,
                reason: "Near-duplicate content detected".into(),
            });
        }

        if let Some(conf) = priors_confidence {
            if conf < 0.3 {
                total -= 0.3;
                components.push(RewardComponent {
                    source: "priors".into(),
                    contribution: -0.3,
                    reason: "Low priors confidence".into(),
                });
            }
        }

        // Lifecycle-aware rewards
        let delta_s = learning_signal.delta_s;
        let delta_c = learning_signal.delta_c;

        match lifecycle {
            LifecycleStage::Infancy => {
                // Favor novelty
                let novelty_reward = delta_s * 0.6;
                total += novelty_reward;
                components.push(RewardComponent {
                    source: "lifecycle_novelty".into(),
                    contribution: novelty_reward,
                    reason: "Infancy stage: rewarding novelty".into(),
                });
            }
            LifecycleStage::Growth => {
                // Balance novelty and coherence
                let balanced = (delta_s * 0.5 + delta_c * 0.5) * 0.4;
                total += balanced;
                components.push(RewardComponent {
                    source: "lifecycle_balanced".into(),
                    contribution: balanced,
                    reason: "Growth stage: balanced reward".into(),
                });
            }
            LifecycleStage::Maturity => {
                // Favor coherence
                let coherence_reward = delta_c * 0.6;
                total += coherence_reward;
                components.push(RewardComponent {
                    source: "lifecycle_coherence".into(),
                    contribution: coherence_reward,
                    reason: "Maturity stage: rewarding coherence".into(),
                });
            }
        }

        SteeringReward {
            value: total.clamp(-1.0, 1.0),
            components,
            lifecycle_stage: lifecycle.clone(),
        }
    }
}
```

### Response Schema Extension

```rust
#[derive(Serialize)]
pub struct StoreMemoryResponse {
    // Existing fields
    pub success: bool,
    pub node_id: Option<NodeId>,
    pub message: String,

    // New UTL fields
    #[serde(skip_serializing_if = "Option::is_none")]
    pub steering_reward: Option<SteeringReward>,

    #[serde(skip_serializing_if = "Option::is_none")]
    pub utl_state: Option<UtlStateSummary>,

    #[serde(skip_serializing_if = "Option::is_none")]
    pub storage_decision: Option<StorageDecision>,
}

pub struct StorageDecision {
    pub should_store: bool,
    pub reason: String,
    pub lifecycle_stage: LifecycleStage,
}
```

### Integration with store_memory Handler

```rust
pub async fn handle_store_memory(
    request: StoreMemoryRequest,
    utl_processor: &UtlProcessor,
    lifecycle_manager: &LifecycleManager,
    steering_hooks: &SteeringHooks,
) -> Result<StoreMemoryResponse, McpError> {
    // 1. Compute UTL for incoming content
    let learning_signal = utl_processor.compute(
        &request.content,
        &request.embedding,
        &request.context,
    )?;

    // 2. Validate priors_vibe_check if provided
    let priors_confidence = request.priors_vibe_check
        .as_ref()
        .map(|p| validate_priors(p, &learning_signal));

    // 3. Check for near-duplicates
    let is_near_duplicate = check_near_duplicate(
        &request.embedding,
        &request.context,
    )?;

    // 4. Get current lifecycle stage
    let lifecycle_stage = lifecycle_manager.current_stage();

    // 5. Compute steering reward
    let steering_reward = steering_hooks.compute_reward(
        &learning_signal,
        &lifecycle_stage,
        request.rationale.is_some(),
        priors_confidence,
        is_near_duplicate,
    );

    // 6. Make storage decision
    let storage_decision = make_storage_decision(
        &learning_signal,
        &lifecycle_stage,
        is_near_duplicate,
    );

    // 7. Store if approved
    let (success, node_id, message) = if storage_decision.should_store {
        let node = store_memory_node(request, &learning_signal)?;
        (true, Some(node.id), "Memory stored successfully".into())
    } else {
        (false, None, storage_decision.reason.clone())
    };

    Ok(StoreMemoryResponse {
        success,
        node_id,
        message,
        steering_reward: Some(steering_reward),
        utl_state: Some(learning_signal.into()),
        storage_decision: Some(storage_decision),
    })
}
```

## Performance Targets
- Total store_memory with UTL: <50ms
- Steering reward computation: <5ms
- Near-duplicate check: <10ms

## Related Tasks
- M05-T22: Provides UtlProcessor for UTL computation
- M05-T19: Provides LifecycleManager for stage-aware decisions
- M05-T36: Provides SteeringHooks for reward integration

---
*Generated: 2026-01-04*
*Module: 05 - UTL Integration*
*Version: 1.0.0*
