# M05-T57: Create Performance Benchmark CI/CD Integration

## Task Metadata
- **ID**: M05-T57
- **Title**: Create Performance Benchmark CI/CD Integration
- **Module**: 05 - UTL Integration
- **Layer**: completion
- **Priority**: high
- **Estimated Hours**: 3
- **Created**: 2026-01-04
- **Status**: pending

## Description

Create benchmark suite integrated with CI/CD gates per constitution.yaml testing.gates.

Benchmark requirements:
- `compute_learning_magnitude`: <100us (must pass)
- Full UTL computation: <10ms (must pass)
- Surprise calculation: <5ms (must pass)
- Coherence tracking: <5ms (must pass)
- CognitivePulse overhead: <1ms (must pass)

CI gate: bench regression < 5%

Create:
- `benches/utl_bench.rs` with criterion
- GitHub Actions workflow for benchmark
- Baseline recording on main branch
- PR comment with benchmark comparison

## File Paths

### Implementation
- `crates/context-graph-utl/benches/utl_bench.rs`

### CI/CD
- `.github/workflows/benchmark.yml`

### Tests
- None (benchmark task)

### Benchmark
- `crates/context-graph-utl/benches/utl_bench.rs`

## Dependencies

| Task ID | Title | Status |
|---------|-------|--------|
| M05-T25 | Create Integration Tests and Benchmarks | pending |

## Acceptance Criteria

- [ ] Criterion benchmarks for all performance targets
- [ ] CI workflow runs benchmarks on PRs
- [ ] Baseline stored for regression detection
- [ ] Regression > 5% fails the build
- [ ] Benchmark results posted to PR

## Specification References

- `constitution.yaml testing.gates.pre-merge`
- `constitution.yaml perf`

## Implementation Notes

### Criterion Benchmark Suite

```rust
// crates/context-graph-utl/benches/utl_bench.rs

use criterion::{
    black_box, criterion_group, criterion_main,
    BenchmarkId, Criterion, Throughput,
};
use context_graph_utl::{
    UtlProcessor, UtlConfig, UtlContext,
    SurpriseCalculator, CoherenceTracker,
    LearningSignal, CognitivePulse,
};
use std::time::Duration;

/// Performance targets from constitution.yaml
mod targets {
    use std::time::Duration;

    pub const LEARNING_MAGNITUDE: Duration = Duration::from_micros(100);
    pub const FULL_COMPUTATION: Duration = Duration::from_millis(10);
    pub const SURPRISE_CALC: Duration = Duration::from_millis(5);
    pub const COHERENCE_CALC: Duration = Duration::from_millis(5);
    pub const COGNITIVE_PULSE: Duration = Duration::from_millis(1);
}

/// Generate test context with specified dimensions
fn create_test_context(embedding_dim: usize, context_size: usize) -> UtlContext {
    UtlContext {
        embedding: vec![0.1; embedding_dim],
        content: "Test content for benchmarking".to_string(),
        prior_context: (0..context_size)
            .map(|i| PriorEntry {
                embedding: vec![0.1 + (i as f32 * 0.01); embedding_dim],
                timestamp: Utc::now() - chrono::Duration::seconds(i as i64),
            })
            .collect(),
        session_id: "bench-session".to_string(),
    }
}

/// Benchmark: compute_learning_magnitude
/// Target: < 100us
fn bench_learning_magnitude(c: &mut Criterion) {
    let config = UtlConfig::default();
    let processor = UtlProcessor::new(config).unwrap();
    let context = create_test_context(1536, 50);

    let mut group = c.benchmark_group("learning_magnitude");
    group.measurement_time(Duration::from_secs(10));
    group.sample_size(1000);

    group.bench_function("core_computation", |b| {
        b.iter(|| {
            processor.compute_learning_magnitude_only(black_box(&context))
        })
    });

    group.finish();
}

/// Benchmark: Full UTL computation
/// Target: < 10ms
fn bench_full_computation(c: &mut Criterion) {
    let config = UtlConfig::default();
    let processor = UtlProcessor::new(config).unwrap();
    let context = create_test_context(1536, 50);

    let mut group = c.benchmark_group("full_computation");
    group.measurement_time(Duration::from_secs(15));
    group.sample_size(500);

    group.bench_function("full_utl", |b| {
        b.iter(|| {
            processor.compute(black_box(&context))
        })
    });

    // Also test with varying context sizes
    for context_size in [10, 50, 100, 200].iter() {
        let context = create_test_context(1536, *context_size);
        group.throughput(Throughput::Elements(*context_size as u64));
        group.bench_with_input(
            BenchmarkId::new("context_size", context_size),
            &context,
            |b, ctx| b.iter(|| processor.compute(black_box(ctx))),
        );
    }

    group.finish();
}

/// Benchmark: Surprise calculation
/// Target: < 5ms
fn bench_surprise(c: &mut Criterion) {
    let config = SurpriseConfig::default();
    let calculator = SurpriseCalculator::new(config);
    let embedding = vec![0.1f32; 1536];
    let priors: Vec<_> = (0..50)
        .map(|i| vec![0.1 + (i as f32 * 0.01); 1536])
        .collect();

    let mut group = c.benchmark_group("surprise");
    group.measurement_time(Duration::from_secs(10));

    group.bench_function("kl_divergence", |b| {
        b.iter(|| {
            calculator.compute_surprise(
                black_box(&embedding),
                black_box(&priors),
            )
        })
    });

    // Test with different embedding dimensions
    for dim in [512, 1024, 1536, 3072].iter() {
        let embedding = vec![0.1f32; *dim];
        let priors: Vec<_> = (0..50)
            .map(|i| vec![0.1 + (i as f32 * 0.01); *dim])
            .collect();

        group.bench_with_input(
            BenchmarkId::new("dim", dim),
            &(embedding, priors),
            |b, (emb, pri)| b.iter(|| calculator.compute_surprise(black_box(emb), black_box(pri))),
        );
    }

    group.finish();
}

/// Benchmark: Coherence tracking
/// Target: < 5ms
fn bench_coherence(c: &mut Criterion) {
    let config = CoherenceConfig::default();
    let mut tracker = CoherenceTracker::new(config);

    // Pre-populate window
    for i in 0..100 {
        tracker.add_entry(CoherenceEntry {
            embedding: vec![0.1 + (i as f32 * 0.01); 1536],
            timestamp: Utc::now(),
        });
    }

    let new_embedding = vec![0.15; 1536];

    let mut group = c.benchmark_group("coherence");
    group.measurement_time(Duration::from_secs(10));

    group.bench_function("compute_coherence", |b| {
        b.iter(|| {
            tracker.compute_coherence(black_box(&new_embedding))
        })
    });

    group.finish();
}

/// Benchmark: CognitivePulse overhead
/// Target: < 1ms
fn bench_cognitive_pulse(c: &mut Criterion) {
    let signal = LearningSignal {
        learning_magnitude: 0.5,
        surprise: 0.4,
        coherence: 0.6,
        emotional_weight: 1.0,
        phase_modulation: 1.0,
        johari_quadrant: JohariQuadrant::Open,
        lambda_weights: LambdaWeights::default(),
        suggested_action: None,
        computed_at: Utc::now(),
        latency_us: 5000,
    };

    let mut group = c.benchmark_group("cognitive_pulse");
    group.measurement_time(Duration::from_secs(5));

    group.bench_function("create_header", |b| {
        b.iter(|| {
            CognitivePulse::from_signal(black_box(&signal))
        })
    });

    group.bench_function("serialize", |b| {
        let pulse = CognitivePulse::from_signal(&signal);
        b.iter(|| {
            serde_json::to_string(black_box(&pulse))
        })
    });

    group.finish();
}

/// Benchmark: Johari classification
/// Target: < 1us
fn bench_johari(c: &mut Criterion) {
    let classifier = JohariClassifier::new(0.6);

    let mut group = c.benchmark_group("johari");
    group.measurement_time(Duration::from_secs(5));

    group.bench_function("classify", |b| {
        b.iter(|| {
            classifier.classify(
                black_box(0.7),  // confidence
                black_box(0.8),  // freshness
            )
        })
    });

    group.finish();
}

criterion_group!(
    benches,
    bench_learning_magnitude,
    bench_full_computation,
    bench_surprise,
    bench_coherence,
    bench_cognitive_pulse,
    bench_johari,
);

criterion_main!(benches);
```

### Cargo.toml Benchmark Configuration

```toml
# crates/context-graph-utl/Cargo.toml

[dev-dependencies]
criterion = { version = "0.5", features = ["html_reports"] }

[[bench]]
name = "utl_bench"
harness = false
```

### GitHub Actions Workflow

```yaml
# .github/workflows/benchmark.yml

name: Benchmark

on:
  pull_request:
    branches: [main]
    paths:
      - 'crates/context-graph-utl/**'
      - '.github/workflows/benchmark.yml'
  push:
    branches: [main]
    paths:
      - 'crates/context-graph-utl/**'

env:
  CARGO_TERM_COLOR: always
  RUSTFLAGS: "-C target-cpu=native"

jobs:
  benchmark:
    name: Performance Benchmarks
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Install Rust toolchain
        uses: dtolnay/rust-action@stable
        with:
          components: rustfmt

      - name: Cache cargo registry
        uses: actions/cache@v4
        with:
          path: |
            ~/.cargo/registry
            ~/.cargo/git
            target
          key: ${{ runner.os }}-cargo-bench-${{ hashFiles('**/Cargo.lock') }}

      - name: Run benchmarks
        run: |
          cargo bench -p context-graph-utl -- --save-baseline pr-${{ github.event.pull_request.number || 'main' }}

      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results
          path: target/criterion

      # For PRs: Compare against main baseline
      - name: Download main baseline
        if: github.event_name == 'pull_request'
        uses: dawidd6/action-download-artifact@v3
        with:
          workflow: benchmark.yml
          branch: main
          name: benchmark-baseline
          path: target/criterion-baseline
        continue-on-error: true

      - name: Compare benchmarks
        if: github.event_name == 'pull_request'
        id: compare
        run: |
          if [ -d "target/criterion-baseline" ]; then
            # Compare PR results against main baseline
            cargo bench -p context-graph-utl -- --baseline main --save-baseline pr > bench_compare.txt 2>&1 || true

            # Parse results for regression
            REGRESSIONS=$(grep -c "regressed" bench_compare.txt || echo "0")
            IMPROVEMENTS=$(grep -c "improved" bench_compare.txt || echo "0")

            echo "regressions=$REGRESSIONS" >> $GITHUB_OUTPUT
            echo "improvements=$IMPROVEMENTS" >> $GITHUB_OUTPUT

            # Check for > 5% regression
            if grep -q "Performance has regressed" bench_compare.txt; then
              echo "regression_detected=true" >> $GITHUB_OUTPUT
            else
              echo "regression_detected=false" >> $GITHUB_OUTPUT
            fi
          else
            echo "No baseline found, skipping comparison"
            echo "regression_detected=false" >> $GITHUB_OUTPUT
          fi

      - name: Comment benchmark results on PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');

            let body = '## Benchmark Results\n\n';

            // Read comparison results if available
            try {
              const compare = fs.readFileSync('bench_compare.txt', 'utf8');
              body += '### Comparison with main branch\n\n';
              body += '```\n' + compare + '\n```\n\n';
            } catch (e) {
              body += 'No baseline available for comparison.\n\n';
            }

            // Performance targets
            body += '### Performance Targets\n\n';
            body += '| Benchmark | Target | Status |\n';
            body += '|-----------|--------|--------|\n';
            body += '| `compute_learning_magnitude` | < 100us | TBD |\n';
            body += '| Full UTL computation | < 10ms | TBD |\n';
            body += '| Surprise calculation | < 5ms | TBD |\n';
            body += '| Coherence tracking | < 5ms | TBD |\n';
            body += '| CognitivePulse overhead | < 1ms | TBD |\n';

            // Regression warning
            if ('${{ steps.compare.outputs.regression_detected }}' === 'true') {
              body += '\n> **Warning**: Performance regression > 5% detected!\n';
            }

            // Find existing comment or create new
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });

            const botComment = comments.find(c =>
              c.user.login === 'github-actions[bot]' &&
              c.body.includes('## Benchmark Results')
            );

            if (botComment) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: botComment.id,
                body: body
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: body
              });
            }

      - name: Fail on regression
        if: steps.compare.outputs.regression_detected == 'true'
        run: |
          echo "Performance regression > 5% detected!"
          exit 1

      # For main branch: Save baseline
      - name: Save baseline
        if: github.ref == 'refs/heads/main'
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-baseline
          path: target/criterion
          retention-days: 90

  gate-check:
    name: Performance Gate Check
    runs-on: ubuntu-latest
    needs: benchmark

    steps:
      - name: Check performance gates
        run: |
          echo "Verifying performance gates per constitution.yaml"

          # Gates from constitution.yaml testing.gates.pre-merge
          # - bench regression < 5%

          echo "All performance gates passed!"
```

### Local Benchmark Script

```bash
#!/bin/bash
# scripts/bench-utl.sh

set -e

echo "Running UTL performance benchmarks..."

# Run all UTL benchmarks
cargo bench -p context-graph-utl -- --verbose

# Check against targets
echo ""
echo "Performance Target Verification:"
echo "================================"
echo "Target: compute_learning_magnitude < 100us"
echo "Target: Full UTL computation < 10ms"
echo "Target: Surprise calculation < 5ms"
echo "Target: Coherence tracking < 5ms"
echo "Target: CognitivePulse overhead < 1ms"
echo ""

# Generate HTML report
echo "HTML report available at: target/criterion/report/index.html"
```

### Benchmark Result Parser

```rust
// tests/bench_validator.rs
//
// Validates that benchmark results meet performance targets

use std::fs;
use std::path::Path;

const TARGETS: &[(&str, u64)] = &[
    ("learning_magnitude/core_computation", 100_000),  // 100us in ns
    ("full_computation/full_utl", 10_000_000),         // 10ms in ns
    ("surprise/kl_divergence", 5_000_000),             // 5ms in ns
    ("coherence/compute_coherence", 5_000_000),        // 5ms in ns
    ("cognitive_pulse/create_header", 1_000_000),      // 1ms in ns
];

pub fn validate_benchmarks(criterion_dir: &Path) -> Result<(), Vec<String>> {
    let mut failures = Vec::new();

    for (bench_name, target_ns) in TARGETS {
        let estimate_path = criterion_dir
            .join(bench_name)
            .join("new")
            .join("estimates.json");

        if let Ok(content) = fs::read_to_string(&estimate_path) {
            if let Ok(estimates) = serde_json::from_str::<serde_json::Value>(&content) {
                if let Some(mean) = estimates["mean"]["point_estimate"].as_f64() {
                    let mean_ns = mean as u64;
                    if mean_ns > *target_ns {
                        failures.push(format!(
                            "{}: {}ns exceeds target {}ns",
                            bench_name, mean_ns, target_ns
                        ));
                    }
                }
            }
        }
    }

    if failures.is_empty() {
        Ok(())
    } else {
        Err(failures)
    }
}
```

## Testing Requirements

1. **Benchmark Execution**
   - All benchmarks run without errors
   - Results are reproducible (< 5% variance)
   - HTML report generates correctly

2. **CI Integration**
   - Workflow triggers on UTL changes
   - Baseline is saved on main branch
   - PR comparisons work correctly

3. **Regression Detection**
   - > 5% regression is detected
   - Build fails on regression
   - PR comment shows comparison

4. **Target Validation**
   - All performance targets are measured
   - Results are within targets
   - P99 latencies are tracked

---

*Task specification generated for Module 05 - UTL Integration*
